#!/usr/bin/env python3
"""
ğŸ” Dual Search Engine for Theology AI Lab v5.1
==============================================
ì´ì¤‘ ê²€ìƒ‰ ì—”ì§„: Vector DB + Archive JSON ë™ì‹œ ê²€ìƒ‰
3ì¤‘ ì–¸ì–´ ì¿¼ë¦¬ í™•ì¥ê³¼ ê²°í•©í•˜ì—¬ ~99% ì»¤ë²„ë¦¬ì§€ ë‹¬ì„±.
"""

import json
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Set
from dataclasses import dataclass, field

logger = logging.getLogger("DualSearch")

# Query Expander ì„í¬íŠ¸
try:
    from utils.query_expander import QueryExpander, get_search_terms
    EXPANDER_AVAILABLE = True
except ImportError:
    EXPANDER_AVAILABLE = False
    logger.warning("QueryExpander not available, using single-language search")


@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ ì•„ì´í…œ"""
    content: str
    source: str
    author: str = "Unknown"
    doc_type: str = "general"
    page: Optional[int] = None
    score: float = 0.0
    method: str = "vector"  # vector, json, hybrid
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "content": self.content[:500] + "..." if len(self.content) > 500 else self.content,
            "source": self.source,
            "author": self.author,
            "doc_type": self.doc_type,
            "page": self.page,
            "score": round(self.score, 3),
            "method": self.method,
        }


class DualSearchEngine:
    """
    ì´ì¤‘ ê²€ìƒ‰ ì—”ì§„: Vector DB + Archive JSON ë™ì‹œ ê²€ìƒ‰.
    """
    
    def __init__(self, 
                 db_path: str, 
                 archive_path: str,
                 use_trilingual: bool = True):
        """
        Args:
            db_path: ChromaDB ê²½ë¡œ
            archive_path: Archive JSON ë””ë ‰í† ë¦¬ ê²½ë¡œ
            use_trilingual: 3ì¤‘ ì–¸ì–´ í™•ì¥ ì‚¬ìš© ì—¬ë¶€
        """
        self.db_path = Path(db_path)
        self.archive_path = Path(archive_path)
        self.use_trilingual = use_trilingual and EXPANDER_AVAILABLE
        
        self.expander = QueryExpander() if self.use_trilingual else None
        self._vector_db = None
        self._embedder = None
        
    def _get_vector_db(self):
        """Lazy load ChromaDB"""
        if self._vector_db is None:
            import chromadb
            client = chromadb.PersistentClient(path=str(self.db_path))
            self._vector_db = client.get_or_create_collection("theology_library")
        return self._vector_db
    
    def _get_embedder(self):
        """Lazy load embedder"""
        if self._embedder is None:
            from sentence_transformers import SentenceTransformer
            self._embedder = SentenceTransformer("BAAI/bge-m3")
        return self._embedder
    
    def search(self, 
               query: str, 
               n_results: int = 10,
               source_filter: Optional[str] = None,
               doc_type_filter: Optional[str] = None,
               tag_filter: Optional[List[str]] = None) -> List[SearchResult]:
        """
        ì´ì¤‘ ê²€ìƒ‰ ì‹¤í–‰.
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            n_results: ê²°ê³¼ ìˆ˜
            source_filter: ì†ŒìŠ¤ í•„í„° (ì˜ˆ: "TDNT", "Barth")
            doc_type_filter: ë„ì„œ ìœ í˜• í•„í„° (dogmatics, dictionary, etc.)
            tag_filter: íƒœê·¸ í•„í„° ë¦¬ìŠ¤íŠ¸
            
        Returns:
            SearchResult ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"ğŸ” Dual Search: '{query}'")
        
        # 1. ì¿¼ë¦¬ í™•ì¥ (3ì¤‘ ì–¸ì–´)
        search_terms = self._expand_query(query)
        logger.info(f"   â””â”€ Search terms: {search_terms[:5]}...")
        
        # 2. Vector DB ê²€ìƒ‰
        vector_results = self._search_vector(search_terms, n_results * 2)
        logger.info(f"   â””â”€ Vector results: {len(vector_results)}")
        
        # 3. Archive JSON ê²€ìƒ‰
        json_results = self._search_archive(search_terms, n_results * 2)
        logger.info(f"   â””â”€ JSON results: {len(json_results)}")
        
        # 4. ê²°ê³¼ ë³‘í•© ë° ì¤‘ë³µ ì œê±°
        merged = self._merge_results(vector_results, json_results)
        
        # 5. í•„í„° ì ìš©
        filtered = self._apply_filters(merged, source_filter, doc_type_filter, tag_filter)
        
        # 6. ì¬ìˆœìœ„í™” ë° ê²°ê³¼ ë°˜í™˜
        return self._rerank(filtered)[:n_results]
    
    def _expand_query(self, query: str) -> List[str]:
        """ì¿¼ë¦¬ë¥¼ ë‹¤êµ­ì–´ë¡œ í™•ì¥"""
        if self.use_trilingual and self.expander:
            return get_search_terms(query)
        return [query]
    
    def _search_vector(self, terms: List[str], n: int) -> List[SearchResult]:
        """Vector DB ê²€ìƒ‰"""
        results = []
        try:
            collection = self._get_vector_db()
            embedder = self._get_embedder()
            
            # ê° ê²€ìƒ‰ì–´ì— ëŒ€í•´ ì„ë² ë”© ìƒì„±
            query_embeddings = embedder.encode(terms[:3]).tolist()  # ìƒìœ„ 3ê°œë§Œ
            
            for i, emb in enumerate(query_embeddings):
                raw = collection.query(
                    query_embeddings=[emb],
                    n_results=n // len(query_embeddings) + 1
                )
                
                if raw['documents'] and raw['documents'][0]:
                    for j, doc in enumerate(raw['documents'][0]):
                        meta = raw['metadatas'][0][j] if raw['metadatas'] else {}
                        results.append(SearchResult(
                            content=doc,
                            source=meta.get('source', 'Unknown'),
                            author=meta.get('author', 'Unknown'),
                            doc_type=meta.get('doc_type', 'general'),
                            page=meta.get('page_number'),
                            score=1.0 - (j * 0.05),  # ìˆœìœ„ ê¸°ë°˜ ì ìˆ˜
                            method="vector",
                            metadata=meta
                        ))
        except Exception as e:
            logger.error(f"Vector search failed: {e}")
        
        return results
    
    def _search_archive(self, terms: List[str], n: int) -> List[SearchResult]:
        """Archive JSON í‚¤ì›Œë“œ ê²€ìƒ‰"""
        results = []
        
        if not self.archive_path.exists():
            return results
        
        # ê²€ìƒ‰ì–´ë¥¼ ì†Œë¬¸ìë¡œ ì •ê·œí™”
        terms_lower = [t.lower() for t in terms]
        
        json_files = list(self.archive_path.glob("*.json"))
        
        for jf in json_files[:50]:  # ìµœëŒ€ 50ê°œ íŒŒì¼
            try:
                with open(jf, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                chunks = data.get('chunks', data) if isinstance(data, dict) else data
                if not isinstance(chunks, list):
                    continue
                
                for chunk in chunks:
                    content = chunk.get('content', chunk.get('text', ''))
                    if not content:
                        continue
                    
                    content_lower = content.lower()
                    
                    # ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
                    score = sum(1 for t in terms_lower if t in content_lower)
                    
                    if score > 0:
                        meta = chunk.get('metadata', chunk)
                        results.append(SearchResult(
                            content=content,
                            source=meta.get('source', jf.stem),
                            author=meta.get('author', 'Unknown'),
                            doc_type=meta.get('doc_type', 'general'),
                            page=meta.get('page_number'),
                            score=score / len(terms),
                            method="json",
                            metadata=meta
                        ))
                        
            except Exception as e:
                logger.debug(f"Error reading {jf}: {e}")
                continue
        
        # ì ìˆ˜ ìˆœ ì •ë ¬
        results.sort(key=lambda x: x.score, reverse=True)
        return results[:n]
    
    def _merge_results(self, 
                       vector: List[SearchResult], 
                       archive: List[SearchResult]) -> List[SearchResult]:
        """ê²°ê³¼ ë³‘í•© ë° ì¤‘ë³µ ì œê±°"""
        seen: Set[str] = set()
        merged = []
        
        # Vector ê²°ê³¼ ìš°ì„ 
        for r in vector:
            key = r.content[:100]  # ì• 100ìë¡œ ì¤‘ë³µ íŒë³„
            if key not in seen:
                seen.add(key)
                merged.append(r)
        
        # Archive ê²°ê³¼ ì¶”ê°€
        for r in archive:
            key = r.content[:100]
            if key not in seen:
                seen.add(key)
                r.method = "hybrid" if r.source in [m.source for m in merged] else "json"
                merged.append(r)
        
        return merged
    
    def _apply_filters(self,
                       results: List[SearchResult],
                       source: Optional[str],
                       doc_type: Optional[str],
                       tags: Optional[List[str]]) -> List[SearchResult]:
        """í•„í„° ì ìš©"""
        filtered = results
        
        if source:
            source_lower = source.lower()
            filtered = [r for r in filtered if source_lower in r.source.lower()]
        
        if doc_type:
            filtered = [r for r in filtered if r.doc_type == doc_type]
        
        if tags:
            tags_lower = [t.lower() for t in tags]
            filtered = [
                r for r in filtered 
                if any(t in str(r.metadata.get('tags', [])).lower() for t in tags_lower)
            ]
        
        return filtered
    
    def _rerank(self, results: List[SearchResult]) -> List[SearchResult]:
        """ê²°ê³¼ ì¬ìˆœìœ„í™” (ê°„ë‹¨í•œ ì ìˆ˜ ê¸°ë°˜)"""
        # Vector ê²°ê³¼ì— ê°€ì‚°ì 
        for r in results:
            if r.method == "vector":
                r.score += 0.2
            elif r.method == "hybrid":
                r.score += 0.3  # ì–‘ìª½ì—ì„œ ë°œê²¬ëœ ê²½ìš° ìµœê³ ì 
        
        results.sort(key=lambda x: x.score, reverse=True)
        return results


# ============================================================
# Convenience Function
# ============================================================
def dual_search(query: str, 
                db_path: str, 
                archive_path: str,
                n_results: int = 10,
                trilingual: bool = True) -> List[Dict]:
    """
    ì´ì¤‘ ê²€ìƒ‰ í¸ì˜ í•¨ìˆ˜.
    
    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        db_path: ChromaDB ê²½ë¡œ
        archive_path: Archive ê²½ë¡œ
        n_results: ê²°ê³¼ ìˆ˜
        trilingual: 3ì¤‘ ì–¸ì–´ í™•ì¥ ì‚¬ìš©
        
    Returns:
        ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
    """
    engine = DualSearchEngine(db_path, archive_path, trilingual)
    results = engine.search(query, n_results)
    return [r.to_dict() for r in results]


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ì´ì¤‘ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
    parser.add_argument("-q", "--query", required=True)
    parser.add_argument("--db", default="./02_Brain/vector_db")
    parser.add_argument("--archive", default="./01_Library/archive")
    parser.add_argument("-n", "--num", type=int, default=5)
    
    args = parser.parse_args()
    
    results = dual_search(args.query, args.db, args.archive, args.num)
    
    for i, r in enumerate(results):
        print(f"\n{'='*60}")
        print(f"[{i+1}] {r['source']} (p.{r['page']}) - {r['method']}")
        print(f"Score: {r['score']}")
        print(f"{r['content']}")
