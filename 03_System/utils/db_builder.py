#!/usr/bin/env python3
"""
ì‹ í•™ ë²¡í„° DB ë¹Œë” (M1 ë§¥ë¶ìš©)
- inbox/ í´ë”ì˜ JSON íŒŒì¼ì„ ChromaDBë¡œ ë²¡í„°í™”
- ì²˜ë¦¬ ì™„ë£Œ ì‹œ JSONì€ archive/ë¡œ ì´ë™ (ê²€ìƒ‰ìš©)
- ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€: ë½ íŒŒì¼ ì‚¬ìš©
"""

import os
import sys
import json
import shutil
import chromadb
from sentence_transformers import SentenceTransformer
from datetime import datetime

from dotenv import load_dotenv

# Load .env explicitly from the kit root (parent of 03_System)
current_dir = os.path.dirname(os.path.abspath(__file__))
# utils -> 03_System -> Theology_AI_Lab (Root)
kit_root = os.path.dirname(os.path.dirname(current_dir)) 
load_dotenv(os.path.join(kit_root, ".env"))

# =========================================================
# âš™ï¸ ì„¤ì • - ê²½ë¡œ íƒì§€ (Env -> Relative -> Default)
# ìš°ì„ ìˆœìœ„: Env > Relative (Standard)
def get_path(env_var, default_rel):
    env_val = os.environ.get(env_var)
    if env_val:
        if env_val.startswith("."):
            return os.path.abspath(os.path.join(kit_root, env_val))
        return env_val
    return os.path.join(kit_root, default_rel)

INBOX_DIR = get_path("INBOX_DIR", "01_Library/inbox")
ARCHIVE_DIR = get_path("ARCHIVE_DIR", "01_Library/archive")
DB_PATH = get_path("CHROMA_DB_DIR", "02_Brain/vector_db")
LOCK_FILE = os.path.join(kit_root, ".db_builder.lock")
# =========================================================

def acquire_lock():
    """ë½ íŒŒì¼ì„ íšë“í•©ë‹ˆë‹¤. ì´ë¯¸ ì‹¤í–‰ ì¤‘ì´ë©´ ì¢…ë£Œ."""
    if os.path.exists(LOCK_FILE):
        # ë½ íŒŒì¼ì´ 1ì‹œê°„ ì´ìƒ ì˜¤ë˜ë˜ì—ˆìœ¼ë©´ ì‚­ì œ (ì¢€ë¹„ ë½ ë°©ì§€)
        if os.path.getmtime(LOCK_FILE) < (datetime.now().timestamp() - 3600):
            os.remove(LOCK_FILE)
            print("âš ï¸ ì˜¤ë˜ëœ ë½ íŒŒì¼ ì‚­ì œë¨")
        else:
            print("ğŸ”’ ë‹¤ë¥¸ db_builder í”„ë¡œì„¸ìŠ¤ê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
            sys.exit(0)
    
    with open(LOCK_FILE, 'w') as f:
        f.write(str(os.getpid()))

def release_lock():
    """ë½ íŒŒì¼ì„ í•´ì œí•©ë‹ˆë‹¤."""
    if os.path.exists(LOCK_FILE):
        os.remove(LOCK_FILE)

def is_already_archived(filename):
    """íŒŒì¼ì´ ì´ë¯¸ archiveì— ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    return os.path.exists(os.path.join(ARCHIVE_DIR, filename))

def build_database():
    # 1. í´ë” í™•ì¸ ë° ìƒì„±
    for path in [INBOX_DIR, ARCHIVE_DIR]:
        if not os.path.exists(path):
            os.makedirs(path)
            print(f"ğŸ“ í´ë” ìƒì„±ë¨: {path}")

    # 2. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ë‹¤êµ­ì–´ ì§€ì›, ì‹ í•™ ë¶„ì•¼ ì¤‘ìš”)
    print("ğŸ§  ëª¨ë¸ ë¡œë”© ì¤‘... (paraphrase-multilingual-MiniLM-L12-v2)")
    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    
    # 3. DB ì—°ê²° (ì—†ìœ¼ë©´ ìë™ ìƒì„±)
    client = chromadb.PersistentClient(path=DB_PATH)
    collection = client.get_or_create_collection(name="theology_library")
    print(f"ğŸ“š í˜„ì¬ DB ë¬¸ì„œ ìˆ˜: {collection.count()}ê°œ")

    # 4. Inbox ìŠ¤ìº”
    json_files = [f for f in os.listdir(INBOX_DIR) if f.endswith('.json')]
    
    if not json_files:
        print("ğŸ“­ Inboxê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. JSON íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”.")
        return

    print(f"ğŸš€ {len(json_files)}ê°œì˜ ìƒˆ íŒŒì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤!")

    # 5. íŒŒì¼ ì²˜ë¦¬ ë£¨í”„
    for filename in json_files:
        # ì´ë¯¸ archiveì— ìˆìœ¼ë©´ ìŠ¤í‚µ (ì¤‘ë³µ ë°©ì§€)
        if is_already_archived(filename):
            print(f"â­ï¸ '{filename}' ì´ë¯¸ archiveì— ì¡´ì¬. ìŠ¤í‚µ.")
            # inboxì—ì„œ ì‚­ì œ
            os.remove(os.path.join(INBOX_DIR, filename))
            continue
            
        file_path = os.path.join(INBOX_DIR, filename)
        source_name = os.path.splitext(filename)[0]
        
        print(f"\nğŸ“„ Processing: {filename} ...")
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                first_char = f.read(1)
                f.seek(0)
                
                documents = []
                ids = []
                metadatas = []
                
                # JSON ë˜ëŠ” JSONL ìë™ ê°ì§€
                if first_char == '[':
                    data = json.load(f)
                else:
                    try:
                        # Try reading as single JSON object first
                        f.seek(0)
                        raw_data = json.load(f)
                        if isinstance(raw_data, dict) and 'chunks' in raw_data:
                            data = raw_data['chunks']
                            print(f"   ğŸ’¡ ì‹ ê·œ í¬ë§· ê°ì§€ ({len(data)} ì²­í¬)")
                        elif isinstance(raw_data, dict):
                            data = [raw_data]
                        else:
                            # Not a dict or list (e.g. primitive), treat as single item? 
                            # But usually db builder expects dicts. 
                            data = [raw_data]
                    except json.JSONDecodeError:
                        # Failed to load as single JSON, try JSONL
                        f.seek(0)
                        data = [json.loads(line) for line in f if line.strip()]

                for item in data:
                    # ê³ ìœ  ID ìƒì„±
                    unique_id = f"{source_name}_{item.get('id', hash(item.get('text', '')))}"
                    
                    # ë©”íƒ€ë°ì´í„° ì •ë¦¬
                    meta = item.get('metadata', {})
                    meta['source'] = source_name
                    meta['indexed_at'] = datetime.now().isoformat()
                    
                    # None ê°’ ë° ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬ (ChromaDB í˜¸í™˜ì„±)
                    for k, v in list(meta.items()):
                        if v is None:
                            meta[k] = ""
                        elif isinstance(v, list):
                            # [v2.0] ë¦¬ìŠ¤íŠ¸ë¥¼ ì‰¼í‘œ êµ¬ë¶„ ë¬¸ìì—´ë¡œ ë³€í™˜
                            meta[k] = ", ".join(str(item) for item in v)
                    
                    documents.append(item['text'])
                    ids.append(unique_id)
                    metadatas.append(meta)

            # ë°°ì¹˜ ì²˜ë¦¬ (10ê°œì”© - ì‚¬ìš©ì í”¼ë“œë°± ë°˜ì˜)
            batch_size = 10
            total_chunks = len(documents)
            print(f"   ğŸ“Š ì´ {total_chunks} ì²­í¬ ë²¡í„°í™” ì‹œì‘...", flush=True)

            for i in range(0, total_chunks, batch_size):
                batch_docs = documents[i : i + batch_size]
                batch_ids = ids[i : i + batch_size]
                batch_meta = metadatas[i : i + batch_size]
                
                # ë²¡í„° ë³€í™˜
                embeddings = model.encode(batch_docs).tolist()
                
                # DBì— upsert
                collection.upsert(
                    ids=batch_ids,
                    documents=batch_docs,
                    embeddings=embeddings,
                    metadatas=batch_meta
                )
                
                # [v2.7.23] ì§„í–‰ë¥  í‘œì‹œ (ë§¤ ë°°ì¹˜ë§ˆë‹¤)
                processed = min(i + batch_size, total_chunks)
                pct = int(processed / total_chunks * 100)
                print(f"   ğŸ”„ ë²¡í„°í™”: {processed}/{total_chunks} ({pct}%)", flush=True)

            # ì™„ë£Œëœ íŒŒì¼ archiveë¡œ ì´ë™
            shutil.move(file_path, os.path.join(ARCHIVE_DIR, filename))
            print(f"ğŸ‰ '{filename}' â†’ archive ì´ë™ ì™„ë£Œ")

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ({filename}): {e}")

    print(f"\nğŸ ì™„ë£Œ! ìµœì¢… DB ë¬¸ì„œ ìˆ˜: {collection.count()}ê°œ")

if __name__ == "__main__":
    try:
        acquire_lock()
        build_database()
    finally:
        release_lock()
