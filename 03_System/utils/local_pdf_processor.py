#!/usr/bin/env python3
"""
ë¡œì»¬ PDF í”„ë¡œì„¸ì„œ - Theology Dictionary Indexing (v2.1)
Colab ì—†ì´ ë¡œì»¬ì—ì„œ PDFë¥¼ ChromaDB JSONìœ¼ë¡œ ë³€í™˜

Features:
- Volume/Lemma ìë™ ì¶”ì¶œ
- í† í° ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹
- ChromaDB í˜¸í™˜ JSON ì¶œë ¥
- [v2.0] í™•ì¥ ë©”íƒ€ë°ì´í„° (category, language, related_lemmas)
- [v2.1] í˜ì´ì§€ ë²ˆí˜¸ ìë™ ê°ì§€ (pdf_page vs print_page)
"""

import os
import re
import json
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from collections import defaultdict

try:
    import tiktoken
except ImportError:
    tiktoken = None

try:
    from pypdf import PdfReader
except ImportError:
    PdfReader = None

try:
    from langchain_text_splitters import RecursiveCharacterTextSplitter
except ImportError:
    RecursiveCharacterTextSplitter = None

try:
    from ebooklib import epub
    from bs4 import BeautifulSoup
    EPUB_AVAILABLE = True
except ImportError:
    EPUB_AVAILABLE = False

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„¤ì •
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ì‚¬ì „ ì•½ì–´ ë§¤ì¹­
DICTIONARY_ABBREVS = {
    "TDNT": "Theological Dictionary of the New Testament",
    "NIDNTT": "New International Dictionary of NT Theology",
    "EDNT": "Exegetical Dictionary of the NT",
    "ThWAT": "Theologisches WÃ¶rterbuch zum Alten Testament",
    "EWNT": "Exegetisches WÃ¶rterbuch zum Neuen Testament",
    "EKL": "Evangelisches Kirchenlexikon",
    "RGG": "Religion in Geschichte und Gegenwart",
    "HWPh": "Historisches WÃ¶rterbuch der Philosophie",
    "TRE": "Theologische RealenzyklopÃ¤die",
    "Theologische": "TRE",
    "KD": "Kirchliche Dogmatik",
}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# [v2.0] í™•ì¥ ë©”íƒ€ë°ì´í„° ìœ í‹¸ë¦¬í‹°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ì¹´í…Œê³ ë¦¬ ì¶”ë¡  íŒ¨í„´
CATEGORY_PATTERNS = {
    "biblical_figures": r"(?:Abraham|Moses|David|Jesus|Paul|Paulus|Jakob|Isaak|Petrus|Johannes|Elijah|Elias)",
    "systematic_theology": r"(?:Gnade|Rechtfertigung|TrinitÃ¤t|SÃ¼nde|ErlÃ¶sung|Sakrament|Taufe|Abendmahl|Eschatologie)",
    "church_history": r"(?:Luther|Calvin|Reformation|Konzil|Augustin|Barth|Schleiermacher|Kirchen(?:geschicht|vÃ¤ter))",
    "philosophy": r"(?:Kant|Hegel|Aristoteles|Platon|Nietzsche|Heidegger|Philosophie|Metaphysik|Ontologie)",
    "biblical_studies": r"(?:Exegese|Hermeneutik|Septuaginta|Pentateuch|Evangelium|Apostel|Propheten)",
    "ethics": r"(?:Ethik|Moral|Tugend|Pflicht|Gewissen|Verantwortung)",
    "practical_theology": r"(?:Predigt|Seelsorge|Liturgie|Gottesdienst|Gemeinde|Mission)",
}


def detect_language(text: str) -> str:
    """
    í…ìŠ¤íŠ¸ì˜ ì£¼ìš” ì–¸ì–´ ê°ì§€

    Returns:
        ì–¸ì–´ ì½”ë“œ: grc (ê·¸ë¦¬ìŠ¤ì–´), heb (íˆë¸Œë¦¬ì–´), de (ë…ì¼ì–´), en (ì˜ì–´), unknown
    """
    # ê·¸ë¦¬ìŠ¤ì–´ ë¬¸ì í™•ì¸
    if re.search(r'[Î±-Ï‰Î¬-ÏÎ‘-Î©]', text):
        return "grc"
    # íˆë¸Œë¦¬ì–´ ë¬¸ì í™•ì¸
    if re.search(r'[×-×ª]', text):
        return "heb"
    # ë…ì¼ì–´ íŠ¹ì„± ë‹¨ì–´
    if re.search(r'\b(der|die|das|und|ist|nicht|auch|mit|fÃ¼r|von)\b', text, re.I):
        return "de"
    # ì˜ì–´ íŠ¹ì„± ë‹¨ì–´
    if re.search(r'\b(the|and|is|of|to|in|that|for|with)\b', text, re.I):
        return "en"
    # í•œêµ­ì–´ í™•ì¸
    if re.search(r'[ê°€-í£]', text):
        return "ko"
    return "unknown"


def extract_category(lemma: str, text: str) -> List[str]:
    """
    í‘œì œì–´ì™€ ë‚´ìš©ì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ë¡ 

    Args:
        lemma: í‘œì œì–´
        text: ì²­í¬ í…ìŠ¤íŠ¸

    Returns:
        ì¹´í…Œê³ ë¦¬ ëª©ë¡ (ì˜ˆ: ["systematic_theology", "church_history"])
    """
    categories = []
    search_text = f"{lemma or ''} {text[:500]}"

    for cat, pattern in CATEGORY_PATTERNS.items():
        if re.search(pattern, search_text, re.I):
            categories.append(cat)

    return categories if categories else ["general"]


def extract_related_lemmas(text: str) -> List[str]:
    """
    í…ìŠ¤íŠ¸ì—ì„œ ê´€ë ¨ í‘œì œì–´ ì¶”ì¶œ

    ì‹ í•™ ì‚¬ì „ì˜ ì°¸ì¡° íŒ¨í„´:
    - "â†’ Liebe"
    - "siehe Glaube"
    - "vgl. Hoffnung"
    - "s. auch Gericht"
    - "cf. Grace"

    Returns:
        ê´€ë ¨ í‘œì œì–´ ëª©ë¡
    """
    patterns = [
        # ë…ì¼ì–´ ì°¸ì¡°
        r'(?:â†’|siehe|vgl\.?|s\.(?:\s*auch)?|vergleiche)\s+([A-ZÃ„Ã–Ãœ][a-zÃ¤Ã¶Ã¼ÃŸ]+)',
        # ì˜ì–´ ì°¸ì¡°
        r'(?:cf\.?|see|see also|compare)\s+([A-Z][a-z]+)',
        # ê·¸ë¦¬ìŠ¤ì–´ ì°¸ì¡° (ëŒ€ë¬¸ìë¡œ ì‹œì‘)
        r'(?:â†’|cf\.?)\s+([Î‘-Î©][Î±-Ï‰]+)',
    ]

    related = set()
    for pattern in patterns:
        matches = re.findall(pattern, text, re.I)
        related.update(matches)

    return list(related)[:10]  # ìµœëŒ€ 10ê°œ

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def tiktoken_len(text: str) -> int:
    """í† í° ìˆ˜ë¥¼ ì •í™•í•˜ê²Œ ì¸¡ì •"""
    try:
        encoding = tiktoken.get_encoding("cl100k_base")
        return len(encoding.encode(text))
    except:
        return len(text) // 4  # ëŒ€ëµì  ì¶”ì •


def extract_dictionary_info(filename: str) -> Tuple[str, Optional[int]]:
    """
    íŒŒì¼ëª…ì—ì„œ ì‚¬ì „ ì•½ì–´ì™€ ê¶Œ ë²ˆí˜¸ ì¶”ì¶œ
    
    Examples:
        'ThWAT_Vol1.pdf' -> ('ThWAT', 1)
        'EWNT_Bd2.pdf' -> ('EWNT', 2)
        'HWPh.pdf' -> ('HWPh', None)
    """
    stem = Path(filename).stem
    
    # ì‚¬ì „ ì•½ì–´ ì¶”ì¶œ
    dict_abbrev = None
    for abbrev, full_name in DICTIONARY_ABBREVS.items():
        if abbrev.lower() in stem.lower():
            # "Theologische" ë§¤ì¹­ ì‹œ "TRE"ë¡œ ë³€í™˜
            dict_abbrev = "TRE" if abbrev == "Theologische" else abbrev
            break
    
    if not dict_abbrev:
        dict_abbrev = stem.split('_')[0].split()[0].upper()
    
    # ê¶Œ ë²ˆí˜¸ ì¶”ì¶œ
    volume_patterns = [
        r'(?:Vol|Bd|Band|ê¶Œ|V)[.\-_\s]?(\d+)',
        r'(\d+)(?:\s*ê¶Œ|\s*Bd)',
        r'[,\s](\d+)[\s,]',
        r'\s(\d+)[\s_-]',
        r'_(\d+)$',
    ]
    
    volume = None
    for pattern in volume_patterns:
        match = re.search(pattern, stem, re.IGNORECASE)
        if match:
            volume = int(match.group(1))
            break
    
    return dict_abbrev, volume


def extract_lemma(text: str) -> Optional[str]:
    """
    í…ìŠ¤íŠ¸ ì‹œì‘ ë¶€ë¶„ì—ì„œ í‘œì œì–´(Lemma) ì¶”ì¶œ
    
    ì‹ í•™ ì‚¬ì „ í‘œì œì–´ íŒ¨í„´:
    - ì „ì²´ ëŒ€ë¬¸ì: "GNADE", "ABRAHAM"
    - ë…ì¼ì–´: "Alkohol und Alkoholismus"
    - ê·¸ë¦¬ìŠ¤/íˆë¸Œë¦¬ì–´: "á¼€Î³Î¬Ï€Î·", "×—Ö¶×¡Ö¶×“"
    """
    sample = text.strip()[:300]
    
    patterns = [
        # ì „ì²´ ëŒ€ë¬¸ì (TDNT ìŠ¤íƒ€ì¼)
        r'^([A-ZÃ„Ã–Ãœ]{2,}(?:\s+[A-ZÃ„Ã–Ãœ]{2,})*)[\.,\s]',
        
        # ë…ì¼ì–´/ì˜ì–´ (HWPh, ThWAT ìŠ¤íƒ€ì¼)
        r'^([A-ZÃ„Ã–Ãœ][a-zÃ¤Ã¶Ã¼ÃŸ]+(?:\s+(?:und|u\.|,|/)\s*[A-ZÃ„Ã–Ãœ]?[a-zÃ¤Ã¶Ã¼ÃŸ]+)*)[\.,]',
        
        # ê·¸ë¦¬ìŠ¤ì–´
        r'^([Î±-Ï‰Î¬-ÏÎ‘-Î©]+)[\s,\.]',
        
        # íˆë¸Œë¦¬ì–´
        r'^([×-×ª]+)[\s,\.]',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, sample)
        if match:
            lemma = match.group(1).strip()
            if 2 <= len(lemma) <= 50:
                return lemma
    
    return None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PDF ì²˜ë¦¬
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def process_file(
    file_path: str,
    text_splitter,
    page_offset: int = 0,
    double_page: bool = False,
    auto_detect_pages: bool = False,
) -> List[Dict[str, Any]]:
    """
    íŒŒì¼(PDF ë˜ëŠ” TXT)ì„ ì²˜ë¦¬í•˜ì—¬ ë©”íƒ€ë°ì´í„°ê°€ í¬í•¨ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

    Args:
        file_path: ì…ë ¥ íŒŒì¼ ê²½ë¡œ
        text_splitter: í…ìŠ¤íŠ¸ ìŠ¤í”Œë¦¬í„°
        page_offset: ìˆ˜ë™ í˜ì´ì§€ ì˜¤í”„ì…‹ (pdf_page - offset = print_page)
        double_page: ìŠ¤í”„ë ˆë“œ(ì–‘ë©´) ëª¨ë“œ
        auto_detect_pages: [v2.1] í˜ì´ì§€ ë²ˆí˜¸ ìë™ ê°ì§€ (OCR ê¸°ë°˜)
    """
    path = Path(file_path)
    suffix = path.suffix.lower()
    
    if suffix == '.pdf':
        return _process_pdf_content(file_path, text_splitter, page_offset, double_page, auto_detect_pages)
    elif suffix == '.txt':
        return _process_text_content(file_path, text_splitter, page_offset)
    elif suffix == '.epub':
        return _process_epub_content(file_path, text_splitter)
    else:
        print(f"      âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {suffix}")
        return []


def _process_epub_content(epub_path: str, text_splitter) -> List[Dict[str, Any]]:
    """
    EPUB íŒŒì¼ ì²˜ë¦¬ - ì±•í„°ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì²­í‚¹
    
    Args:
        epub_path: EPUB íŒŒì¼ ê²½ë¡œ
        text_splitter: í…ìŠ¤íŠ¸ ìŠ¤í”Œë¦¬í„°
    
    Returns:
        ì²­í¬ ë¦¬ìŠ¤íŠ¸ (ë©”íƒ€ë°ì´í„° í¬í•¨)
    """
    if not EPUB_AVAILABLE:
        print("      âŒ EPUB ì²˜ë¦¬ë¥¼ ìœ„í•´ 'ebooklib'ê³¼ 'beautifulsoup4'ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”:")
        print("         pip install ebooklib beautifulsoup4")
        return []
    
    filename = Path(epub_path).name
    dict_abbrev, volume = extract_dictionary_info(filename)
    
    print(f"   ğŸ“– {filename}")
    print(f"      í˜•ì‹: EPUB, ì†ŒìŠ¤: {dict_abbrev}")
    
    try:
        book = epub.read_epub(epub_path)
    except Exception as e:
        print(f"      âŒ EPUB ì½ê¸° ì˜¤ë¥˜: {e}")
        return []
    
    # ì±•í„°ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    chapters_data = []
    chapter_num = 0
    
    for item in book.get_items():
        if item.get_type() == epub.ITEM_DOCUMENT:
            chapter_num += 1
            content = item.get_content().decode('utf-8', errors='ignore')
            soup = BeautifulSoup(content, 'html.parser')
            
            # HTML íƒœê·¸ ì œê±°, í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
            text = soup.get_text(separator='\n', strip=True)
            
            if text.strip():
                chapters_data.append({
                    'chapter_num': chapter_num,
                    'chapter_id': item.get_id(),
                    'text': text,
                    'char_start': sum(len(c['text']) for c in chapters_data)
                })
    
    if not chapters_data:
        print(f"      âš ï¸ EPUBì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    print(f"      ğŸ“š {len(chapters_data)}ê°œ ì±•í„° ë°œê²¬")
    
    # ì „ì²´ í…ìŠ¤íŠ¸ í†µí•© ë° ì²­í‚¹
    full_text = "\n\n".join(c['text'] for c in chapters_data)
    chunks = text_splitter.split_text(full_text)
    
    # í‘œì œì–´ ê°ì§€
    chunk_lemmas = []
    current_lemma = None
    for chunk in chunks:
        detected = extract_lemma(chunk)
        if detected:
            current_lemma = detected
        chunk_lemmas.append(current_lemma)
    
    # í‘œì œì–´ë³„ ì²­í¬ ìˆ˜ ê³„ì‚°
    lemma_chunk_counts = defaultdict(int)
    for lemma in chunk_lemmas:
        if lemma:
            lemma_chunk_counts[lemma] += 1
    
    # ì²­í¬ ë©”íƒ€ë°ì´í„° ìƒì„±
    result = []
    char_position = 0
    lemma_current_index = defaultdict(int)
    
    for i, chunk in enumerate(chunks):
        chunk_mid = char_position + len(chunk) // 2
        
        # í•´ë‹¹ ì±•í„° ì°¾ê¸°
        chapter_num = 1
        for ch in chapters_data:
            if chunk_mid >= ch['char_start']:
                chapter_num = ch['chapter_num']
            else:
                break
        
        # í‘œì œì–´ ì •ë³´
        lemma = chunk_lemmas[i]
        if lemma:
            lemma_current_index[lemma] += 1
            lemma_idx = lemma_current_index[lemma]
            lemma_total = lemma_chunk_counts[lemma]
        else:
            lemma_idx = None
            lemma_total = None
        
        # í™•ì¥ ë©”íƒ€ë°ì´í„°
        language = detect_language(chunk)
        categories = extract_category(lemma, chunk)
        related = extract_related_lemmas(chunk)
        
        metadata = {
            "source": dict_abbrev,
            "filename": filename,
            "chunk_id": f"chunk_{i}",
            "chapter": chapter_num,
            "total_chapters": len(chapters_data),
            "chunk_tokens": tiktoken_len(chunk),
            "volume": volume,
            "lemma": lemma,
            "lemma_chunk_index": lemma_idx,
            "lemma_total_chunks": lemma_total,
            "file_type": "epub",
            "language": language,
            "category": categories,
            "related_lemmas": related,
        }
        
        # ID ìƒì„±
        chunk_id = f"{dict_abbrev}"
        if volume:
            chunk_id += f"_{volume}"
        if lemma:
            chunk_id += f"_{lemma[:20]}"
        chunk_id += f"_{i:04d}"
        
        result.append({
            "id": chunk_id,
            "text": chunk,
            "metadata": metadata
        })
        
        char_position += len(chunk)
    
    unique_lemmas = len([l for l in set(chunk_lemmas) if l])
    print(f"      âœ… {len(result):,}ê°œ ì²­í¬ ìƒì„± ({unique_lemmas}ê°œ í‘œì œì–´ ê°ì§€)")
    
    return result

def _process_text_content(txt_path: str, text_splitter, page_offset: int) -> List[Dict[str, Any]]:
    """TXT íŒŒì¼ ì²˜ë¦¬ (í˜ì´ì§€ êµ¬ë¶„ ì—†ìŒ ë˜ëŠ” ê°„ë‹¨í•œ êµ¬ë¶„)"""
    try:
        with open(txt_path, 'r', encoding='utf-8') as f:
            text = f.read()
    except Exception as e:
        print(f"      âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
        return []

    # í…ìŠ¤íŠ¸ ì²­í‚¹
    chunks = text_splitter.split_text(text)
    
    # ë©”íƒ€ë°ì´í„° ìƒì„± (TXTëŠ” í˜ì´ì§€ ê°œë…ì´ ëª¨í˜¸í•˜ë¯€ë¡œ ì „ì²´ 1í˜ì´ì§€ë¡œ ê°€ì •í•˜ê±°ë‚˜ ë³„ë„ ë¡œì§ í•„ìš”)
    filename = Path(txt_path).name
    dict_abbrev, volume = extract_dictionary_info(filename)
    
    result = []
    chunk_lemmas = []
    
    # 1. í‘œì œì–´ ê°ì§€
    current_lemma = None
    for chunk in chunks:
        detected = extract_lemma(chunk)
        if detected:
            current_lemma = detected
        chunk_lemmas.append(current_lemma)
        
    # 2. í‘œì œì–´ ì¹´ìš´íŠ¸
    lemma_chunk_counts = defaultdict(int)
    for lemma in chunk_lemmas:
        if lemma: lemma_chunk_counts[lemma] += 1
        
    lemma_current_index = defaultdict(int)
    
    for i, chunk in enumerate(chunks):
        lemma = chunk_lemmas[i]
        if lemma:
            lemma_current_index[lemma] += 1
            lemma_idx = lemma_current_index[lemma]
            lemma_total = lemma_chunk_counts[lemma]
        else:
            lemma_idx = None
            lemma_total = None
            
        # [v2.0] í™•ì¥ ë©”íƒ€ë°ì´í„°
        language = detect_language(chunk)
        categories = extract_category(lemma, chunk)
        related = extract_related_lemmas(chunk)

        metadata = {
            "source": dict_abbrev,
            "filename": filename,
            "chunk_id": f"chunk_{i}",
            "page_number": 1,  # TXTëŠ” ê¸°ë³¸ 1
            "pdf_page": 1,
            "total_pages": 1,
            "chunk_tokens": tiktoken_len(chunk),
            "volume": volume,
            "lemma": lemma,
            "lemma_chunk_index": lemma_idx,
            "lemma_total_chunks": lemma_total,
            "is_spread": False,
            # [v2.0] í™•ì¥ í•„ë“œ
            "language": language,
            "category": categories,
            "related_lemmas": related,
        }
        
        chunk_id = f"{dict_abbrev}"
        if volume: chunk_id += f"_{volume}"
        if lemma: chunk_id += f"_{lemma[:20]}"
        chunk_id += f"_{i:04d}"
        
        result.append({
            "id": chunk_id,
            "text": chunk,
            "metadata": metadata
        })
        
    print(f"      âœ… {len(result):,}ê°œ í…ìŠ¤íŠ¸ ì²­í¬ ìƒì„±")
    return result

def _process_pdf_content(
    pdf_path: str,
    text_splitter,
    page_offset: int = 0,
    double_page: bool = False,
    auto_detect_pages: bool = False,
) -> List[Dict[str, Any]]:
    """
    PDF ë‚´ìš©ì„ ì²˜ë¦¬í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜

    [v2.1] í˜ì´ì§€ ë§¤í•‘ ìš°ì„ ìˆœìœ„:
      1. .mapping.json íŒŒì¼ (ì‚¬ìš©ì ìˆ˜ë™ ë§¤í•‘)
      2. auto_detect_pages=True (OCR ìë™ ê°ì§€)
      3. page_offset (ë‹¨ìˆœ ì˜¤í”„ì…‹)
    """
    # [v2.1] í˜ì´ì§€ ë§¤í•‘ ë¡œë“œ
    page_mapping = {}  # pdf_page â†’ print_page
    mapping_source = None

    # 1ìˆœìœ„: .mapping.json íŒŒì¼ í™•ì¸
    try:
        from page_mapping_loader import load_mapping_file
        mapping_result = load_mapping_file(pdf_path)
        if mapping_result:
            page_mapping = mapping_result.page_map
            mapping_source = f"ë§¤í•‘ íŒŒì¼ ({mapping_result.info})"
            print(f"      âœ… {mapping_source}")
    except ImportError:
        pass
    except Exception as e:
        print(f"      âš ï¸ ë§¤í•‘ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")

    # 2ìˆœìœ„: ìë™ ê°ì§€ (ë§¤í•‘ íŒŒì¼ ì—†ì„ ì‹œ)
    if not page_mapping and auto_detect_pages:
        try:
            from page_number_detector import PageNumberDetector
            detector = PageNumberDetector()
            results = detector.detect_page_numbers(pdf_path, sample_pages=30, use_ocr=True)
            page_mapping = detector.create_offset_map(results)
            mapping_source = "ìë™ ê°ì§€"
            print(f"      âœ¨ í˜ì´ì§€ ìë™ ê°ì§€ ì™„ë£Œ: {len(page_mapping)}í˜ì´ì§€ ë§¤í•‘ë¨")
        except ImportError:
            print(f"      âš ï¸ page_number_detector ëª¨ë“ˆ ì—†ìŒ")
        except Exception as e:
            print(f"      âš ï¸ í˜ì´ì§€ ê°ì§€ ì‹¤íŒ¨: {e}, ìˆ˜ë™ ì˜¤í”„ì…‹ ì‚¬ìš©")
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ğŸ“– Double Page / Layout ì²˜ë¦¬ (TRE Bd.4 ì „ìš© ë§¤í•‘ í¬í•¨)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def get_tre_bd4_pages(pdf_page: int) -> Tuple[Optional[int], Optional[int]]:
        """TRE Bd.4 ì „ìš© ë¶ˆê·œì¹™ ë§¤í•‘ ë¡œì§"""
        if pdf_page < 2: return None, None
        if pdf_page == 2: return None, 1
        if 3 <= pdf_page <= 88:
            return (pdf_page - 2) * 2, (pdf_page - 2) * 2 + 1
        if pdf_page == 89: return 174, None  # Right is blank
        if pdf_page == 90: return None, 175  # Left is blank
        if 91 <= pdf_page <= 266:
            return (pdf_page - 3) * 2, (pdf_page - 3) * 2 + 1
        if pdf_page == 267: return 528, None  # Right is Tafel
        if pdf_page == 268: return None, 529  # Left is Tafel
        if pdf_page >= 269:
            return (pdf_page - 4) * 2, (pdf_page - 4) * 2 + 1
        return None, None

    def get_tre_bd5_pages(pdf_page: int) -> Tuple[Optional[int], Optional[int]]:
        """TRE Bd.5 ì „ìš© ë¶ˆê·œì¹™ ë§¤í•‘ ë¡œì§"""
        if pdf_page < 2: return None, None
        if pdf_page == 2: return None, 1
        if 3 <= pdf_page <= 125:
            return (pdf_page - 2) * 2, (pdf_page - 2) * 2 + 1
        if pdf_page == 126: return 248, None
        if pdf_page == 127: return None, None
        if pdf_page == 128: return None, 249
        if 129 <= pdf_page <= 259:
            return (pdf_page - 4) * 2, (pdf_page - 4) * 2 + 1
        if pdf_page == 260: return 512, None
        if 261 <= pdf_page <= 262: return None, None
        if pdf_page == 263: return None, 513
        if pdf_page >= 264:
            return (pdf_page - 7) * 2, (pdf_page - 7) * 2 + 1
        return None, None

    def get_kd_ii2_page(pdf_page: int) -> Optional[int]:
        """KD II/2 ì „ìš© êµ¬ê°„ë³„ ì˜¤í”„ì…‹ ë§¤í•‘"""
        if pdf_page < 10: return None
        if 10 <= pdf_page <= 716:
            return pdf_page - 9
        if 717 <= pdf_page <= 874:
            return pdf_page - 10
        if 875 <= pdf_page <= 885:
            return pdf_page - 12
        if pdf_page >= 886:
            return pdf_page - 11
        return None

    def get_kd_iv4_page(pdf_page: int) -> Optional[int]:
        """KD IV/4 ì „ìš© ì˜¤í”„ì…‹ ë§¤í•‘ (PDF 13 -> ì¢…ì´ 2, ì˜¤í”„ì…‹ 11)"""
        if pdf_page < 13: return None
        return pdf_page - 11

    def split_double_page(page) -> Tuple[str, str]:
        """PDF í˜ì´ì§€ë¥¼ ì¢Œ/ìš° í…ìŠ¤íŠ¸ë¡œ ë¶„í•  (ì¢Œí‘œ ê¸°ë°˜)"""
        left_parts = []
        right_parts = []
        middle = float(page.mediabox.width) / 2
        
        # í…ìŠ¤íŠ¸ ì¡°ê°ê³¼ ì¢Œí‘œ ìˆ˜ì§‘
        parts = []
        def visitor(text, cm, tm, font_dict, font_size):
            if text.strip():
                parts.append({
                    'text': text,
                    'x': tm[4],
                    'y': tm[5]
                })
        
        page.extract_text(visitor_text=visitor)
        
        # Yì¢Œí‘œ(ìœ„->ì•„ë˜) ìˆœìœ¼ë¡œ ì •ë ¬ (pypdf ì¶”ì¶œ ìˆœì„œê°€ ê¼¬ì¼ ìˆ˜ ìˆìŒ)
        parts.sort(key=lambda p: (-p['y'], p['x']))
        
        for p in parts:
            if p['x'] < middle:
                left_parts.append(p['text'])
            else:
                right_parts.append(p['text'])
                
        return "".join(left_parts), "".join(right_parts)

    filename = Path(pdf_path).name
    dict_abbrev, volume = extract_dictionary_info(filename)
    is_tre_bd4 = (dict_abbrev == "TRE" and volume == 4)
    is_tre_bd5 = (dict_abbrev == "TRE" and volume == 5)
    stem_lower = Path(filename).stem.lower()
    is_kd_ii2 = ("kd" in stem_lower and ("ii-2" in stem_lower or "ii.2" in stem_lower or "ii_2" in stem_lower))
    is_kd_iv4 = ("kd" in stem_lower and ("iv-4" in stem_lower or "iv.4" in stem_lower or "iv_4" in stem_lower))
    
    print(f"   ğŸ“– {filename}")
    print(f"      ì‚¬ì „: {dict_abbrev}, ê¶Œ: {volume or 'ë‹¨ê¶Œ'}")
    if double_page:
        mode_str = "TRE Bd.4 íŠ¹ìˆ˜ ë§¤í•‘" if is_tre_bd4 else ("TRE Bd.5 íŠ¹ìˆ˜ ë§¤í•‘" if is_tre_bd5 else "ì¼ë°˜ ìŠ¤í”„ë ˆë“œ")
        print(f"      âœ¨ ìŠ¤í”„ë ˆë“œ ëª¨ë“œ í™œì„±í™” ({mode_str})")
    elif is_kd_ii2:
        print(f"      âœ¨ KD II.2 íŠ¹ìˆ˜ êµ¬ê°„ ë§¤í•‘ í™œì„±í™”")
    elif is_kd_iv4:
        print(f"      âœ¨ KD IV/4 ê¸°ë³¸ ì˜¤í”„ì…‹(-11) ë§¤í•‘ í™œì„±í™”")
    
    # PDF ì½ê¸°
    reader = PdfReader(pdf_path)
    
    # í˜ì´ì§€ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
    pages_data = []
    total_pages = len(reader.pages)
    
    for page_num, page in enumerate(reader.pages, 1):
        # 10í˜ì´ì§€ë§ˆë‹¤ ë˜ëŠ” ë§ˆì§€ë§‰ í˜ì´ì§€ì— ì§„í–‰ë¥  ì¶œë ¥
        if page_num % 10 == 0 or page_num == total_pages:
            print(f"[PROGRESS] {int(page_num / total_pages * 50)}% (í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...)")
        if double_page:
            left_txt, right_txt = split_double_page(page)
            
            # í˜ì´ì§€ ë²ˆí˜¸ ê²°ì •
            if is_tre_bd4:
                lp, rp = get_tre_bd4_pages(page_num)
            elif is_tre_bd5:
                lp, rp = get_tre_bd5_pages(page_num)
            else:
                # ê¸°ë³¸ ìŠ¤í”„ë ˆë“œ ê³µì‹
                lp = (page_num - page_offset) * 2 - 1
                rp = (page_num - page_offset) * 2
            
            if left_txt.strip() and lp:
                pages_data.append({
                    'page_num': lp,
                    'pdf_page': page_num,
                    'text': left_txt,
                    'char_start': sum(len(p['text']) for p in pages_data)
                })
            if right_txt.strip() and rp:
                pages_data.append({
                    'page_num': rp,
                    'pdf_page': page_num,
                    'text': right_txt,
                    'char_start': sum(len(p['text']) for p in pages_data)
                })
        else:
            text = page.extract_text() or ""
            if text.strip():
                # KD II.2 íŠ¹ìˆ˜ ë§¤í•‘ ë˜ëŠ” ì¼ë°˜ ì˜¤í”„ì…‹ ë§¤í•‘ ì ìš©
                if is_kd_ii2:
                    p_num = get_kd_ii2_page(page_num)
                elif is_kd_iv4:
                    p_num = get_kd_iv4_page(page_num)
                else:
                    p_num = max(1, page_num - page_offset)
                
                if p_num:
                    pages_data.append({
                        'page_num': p_num,
                        'pdf_page': page_num,
                        'text': text,
                        'char_start': sum(len(p['text']) for p in pages_data)
                    })
    
    if not pages_data:
        print(f"      âš ï¸  ë¹ˆ PDF")
        return []
    
    # í…ìŠ¤íŠ¸ í†µí•© ë° ì²­í‚¹
    full_text = "\n\n".join(p['text'] for p in pages_data)
    print(f"[PROGRESS] 60% (ì²­í‚¹ ì‹œì‘...)")
    chunks = text_splitter.split_text(full_text)
    
    # í‘œì œì–´ ê°ì§€ (1ì°¨ íŒ¨ìŠ¤)
    chunk_lemmas = []
    current_lemma = None
    for chunk in chunks:
        detected = extract_lemma(chunk)
        if detected:
            current_lemma = detected
        chunk_lemmas.append(current_lemma)
    
    # í‘œì œì–´ë³„ ì²­í¬ ìˆ˜ ê³„ì‚° (2ì°¨ íŒ¨ìŠ¤)
    lemma_chunk_counts = defaultdict(int)
    for lemma in chunk_lemmas:
        if lemma:
            lemma_chunk_counts[lemma] += 1
    
    # ì²­í¬ ë©”íƒ€ë°ì´í„° ìƒì„± (3ì°¨ íŒ¨ìŠ¤)
    result = []
    char_position = 0
    lemma_current_index = defaultdict(int)
    
    for i, chunk in enumerate(chunks):
        chunk_mid = char_position + len(chunk) // 2
        
        # í˜ì´ì§€ ë²ˆí˜¸ ë° ì›ë³¸ PDF í˜ì´ì§€ ì°¾ê¸°
        page_num = 1
        pdf_page = 1
        for p in pages_data:
            if chunk_mid >= p['char_start']:
                page_num = p['page_num']
                pdf_page = p.get('pdf_page', page_num)
            else:
                break
        
        # í‘œì œì–´ ì •ë³´
        lemma = chunk_lemmas[i]
        if lemma:
            lemma_current_index[lemma] += 1
            lemma_idx = lemma_current_index[lemma]
            lemma_total = lemma_chunk_counts[lemma]
        else:
            lemma_idx = None
            lemma_total = None
        
        # [v2.0] í™•ì¥ ë©”íƒ€ë°ì´í„°
        language = detect_language(chunk)
        categories = extract_category(lemma, chunk)
        related = extract_related_lemmas(chunk)

        # [v2.1] ì‹¤ì œ ì¸ì‡„ë³¸ í˜ì´ì§€ ë²ˆí˜¸ ê²°ì •
        # ìš°ì„ ìˆœìœ„: ìë™ ê°ì§€ > ìˆ˜ë™ ì˜¤í”„ì…‹ > ê¸°ì¡´ page_num
        if page_mapping and pdf_page in page_mapping:
            print_page = page_mapping[pdf_page]
        elif page_offset:
            print_page = max(1, pdf_page - page_offset)
        else:
            print_page = page_num  # ê¸°ì¡´ ë¡œì§ ìœ ì§€

        # ë©”íƒ€ë°ì´í„°
        metadata = {
            "source": dict_abbrev,
            "filename": filename,
            "chunk_id": f"chunk_{i}",
            "page_number": page_num,       # ê¸°ì¡´ í˜¸í™˜ìš© (deprecated)
            "pdf_page": pdf_page,          # PDF ë¬¼ë¦¬ì  í˜ì´ì§€
            "print_page": print_page,      # [v2.1] ì‹¤ì œ ì¸ì‡„ë³¸ í˜ì´ì§€
            "total_pages": len(reader.pages),
            "chunk_tokens": tiktoken_len(chunk),
            "volume": volume,
            "lemma": lemma,
            "lemma_chunk_index": lemma_idx,
            "lemma_total_chunks": lemma_total,
            "is_spread": double_page,
            # [v2.0] í™•ì¥ í•„ë“œ
            "language": language,
            "category": categories,
            "related_lemmas": related,
        }
        
        # ID ìƒì„±
        chunk_id = f"{dict_abbrev}"
        if volume:
            chunk_id += f"_{volume}"
        if lemma:
            chunk_id += f"_{lemma[:20]}"
        chunk_id += f"_{i:04d}"
        
        result.append({
            "id": chunk_id,
            "text": chunk,
            "metadata": metadata
        })
        
        char_position += len(chunk)
        
        # ì²­í‚¹ ì§„í–‰ë¥  (60% ~ 100%)
        if i % 50 == 0 or i == len(chunks) - 1:
            prog = 60 + int((i + 1) / len(chunks) * 40)
            print(f"[PROGRESS] {prog}% (ì²­í¬ ë©”íƒ€ë°ì´í„° ìƒì„± ì¤‘...)")
    
    # í†µê³„
    unique_lemmas = len([l for l in set(chunk_lemmas) if l])
    print(f"      âœ… {len(result):,}ê°œ ì²­í¬ ìƒì„± ({unique_lemmas}ê°œ í‘œì œì–´ ê°ì§€)")
    
    return result


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë©”ì¸ ì‹¤í–‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description='PDFë¥¼ ChromaDBìš© JSONìœ¼ë¡œ ë³€í™˜'
    )
    parser.add_argument('input', help='PDF íŒŒì¼ ë˜ëŠ” í´ë” ê²½ë¡œ')
    parser.add_argument('-o', '--output', default=os.path.expanduser("~/Desktop/MS_Brain.nosync/300 Tech/320 Coding/Projects.nosync/Theology_Project.nosync/inbox"),
                        help='ì¶œë ¥ í´ë” (ê¸°ë³¸: inbox)')
    parser.add_argument('--chunk-size', type=int, default=2800,
                        help='ì²­í¬ í¬ê¸° (ë¬¸ì ìˆ˜)')
    parser.add_argument('--overlap', type=int, default=560,
                        help='ì²­í¬ ì˜¤ë²„ë© (ë¬¸ì ìˆ˜)')
    parser.add_argument('--page-offset', type=int, default=0,
                        help='PDFí˜ì´ì§€ - ì˜¤í”„ì…‹ = ì¢…ì´ì±… í˜ì´ì§€ (ì˜ˆ: TREëŠ” 2)')
    parser.add_argument('--double-page', action='store_true',
                        help='PDF 1í˜ì´ì§€ì— ì¢…ì´ì±… 2í˜ì´ì§€ê°€ í¬í•¨ëœ ê²½ìš°(ìŠ¤í”„ë ˆë“œ)')
    parser.add_argument('--auto-detect', action='store_true',
                        help='[v2.1] í˜ì´ì§€ ë²ˆí˜¸ ìë™ ê°ì§€ (OCR ê¸°ë°˜)')

    args = parser.parse_args()
    
    # ì¶œë ¥ í´ë” ìƒì„±
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    if RecursiveCharacterTextSplitter is None:
        print("âŒ 'langchain-text-splitters' íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        sys.exit(1)

    # í…ìŠ¤íŠ¸ ìŠ¤í”Œë¦¬í„° ì„¤ì •
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=args.chunk_size,
        chunk_overlap=args.overlap,
        separators=["\n\n\n", "\n\n", "\n", ". ", "! ", "? ", "; ", " ", ""],
        length_function=tiktoken_len,
        is_separator_regex=False,
    )
    
    print("=" * 60)
    print("ğŸ“š ë¡œì»¬ PDF â†’ ChromaDB JSON ë³€í™˜")
    print("=" * 60)
    print(f"\nì…ë ¥: {args.input}")
    print(f"ì¶œë ¥: {output_dir}")
    print(f"ì²­í‚¹: ~{args.chunk_size//4} í† í° (ì˜¤ë²„ë© ~{args.overlap//4})\n")
    
    # ì…ë ¥ ê²½ë¡œ í™•ì¸
    input_path = Path(args.input)
    
    if input_path.is_file():
        input_files = [input_path]
    elif input_path.is_dir():
        input_files = list(input_path.glob("*.pdf")) + list(input_path.glob("*.txt"))
    else:
        print(f"âŒ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.input}")
        return
    
    if not input_files:
        print("âŒ ì²˜ë¦¬í•  íŒŒì¼(PDF/TXT)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“‚ ë°œê²¬ëœ íŒŒì¼: {len(input_files)}ê°œ\n")
    
    # PDF ì²˜ë¦¬ ë° JSON ì €ì¥
    print("=" * 60)
    print("ğŸš€ PDF ì²˜ë¦¬ ë° JSON ë³€í™˜ ì‹œì‘")
    print("=" * 60)

    for i, input_file in enumerate(input_files, 1):
        filename = input_file.name
        print(f"\n[{i}/{len(input_files)}] ğŸ“„ {filename}")
        
        try:
            chunks = process_file(
                str(input_file),
                text_splitter,
                args.page_offset,
                args.double_page,
                args.auto_detect,
            )
            
            if chunks:
                # ì¶œë ¥ íŒŒì¼ëª… ê²°ì • (ì…ë ¥ íŒŒì¼ëª… ìœ ì§€)
                output_filename = input_file.stem + ".json"
                output_file = output_dir / output_filename
                
                # ë©”íƒ€ë°ì´í„° ì •ì œ (None ì œê±°)
                for chunk in chunks:
                    clean_meta = {
                        k: (v if v is not None else "")
                        for k, v in chunk['metadata'].items()
                    }
                    chunk['metadata'] = clean_meta

                # JSON ì €ì¥
                with open(output_file, 'w', encoding='utf-8') as f:
                    # DB Builderê°€ ê¸°ëŒ€í•˜ëŠ” í¬ë§·: list of dicts, or dict with 'chunks' list
                    # ì—¬ê¸°ì„œëŠ” list of dictsë¡œ ì €ì¥
                    f.write(json.dumps(chunks, ensure_ascii=False, indent=2))
                
            if chunks:
                # ...
                print(f"   ğŸ’¾ ì €ì¥ ì™„ë£Œ: {output_filename}")
                print(f"   ğŸ“Š ì²­í¬ ìˆ˜: {len(chunks):,}ê°œ")
            else:
                print(f"   âš ï¸  ìƒì„±ëœ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤. (ë¹ˆ íŒŒì¼ ë˜ëŠ” OCR í•„ìš”)")
                sys.exit(1) # íŒŒì´í”„ë¼ì¸ì— ì‹¤íŒ¨ ì•Œë¦¼

        except Exception as e:
            print(f"      âŒ ì˜¤ë¥˜: {e}")
            sys.exit(1)

    print("\n" + "=" * 60)
    print("ğŸ‰ ë³€í™˜ ì™„ë£Œ!")
    
    print("\n" + "=" * 60)
    print("ğŸ‰ ì™„ë£Œ!")
    print("=" * 60)
    print(f"\në‹¤ìŒ ë‹¨ê³„:")
    print(f"  cd .")
    print(f"  source venv.nosync/bin/activate")
    print(f"  python db_builder.py")


if __name__ == "__main__":
    main()
