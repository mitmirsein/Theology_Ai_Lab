#!/usr/bin/env python3
"""
ë¡œì»¬ PDF í”„ë¡œì„¸ì„œ - Theology Dictionary Indexing
Colab ì—†ì´ ë¡œì»¬ì—ì„œ PDFë¥¼ ChromaDB JSONìœ¼ë¡œ ë³€í™˜

Features:
- Volume/Lemma ìë™ ì¶”ì¶œ
- í† í° ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹
- ChromaDB í˜¸í™˜ JSON ì¶œë ¥
"""

import os
import re
import json
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from collections import defaultdict
import tiktoken
from pypdf import PdfReader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„¤ì •
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ì‚¬ì „ ì•½ì–´ ë§¤ì¹­
DICTIONARY_ABBREVS = {
    "TDNT": "Theological Dictionary of the New Testament",
    "NIDNTT": "New International Dictionary of NT Theology",
    "EDNT":" Exegetical Dictionary of the NT",
    "ThWAT": "Theologisches WÃ¶rterbuch zum Alten Testament",
    "EWNT": "Exegetisches WÃ¶rterbuch zum Neuen Testament",
    "EKL": "Evangelisches Kirchenlexikon",
    "RGG": "Religion in Geschichte und Gegenwart",
    "HWPh": "Historisches WÃ¶rterbuch der Philosophie",
    "TRE": "Theologische RealenzyklopÃ¤die",
    "Theologische": "TRE",
    "KD": "Kirchliche Dogmatik",
}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def tiktoken_len(text: str) -> int:
    """í† í° ìˆ˜ë¥¼ ì •í™•í•˜ê²Œ ì¸¡ì •"""
    try:
        encoding = tiktoken.get_encoding("cl100k_base")
        return len(encoding.encode(text))
    except:
        return len(text) // 4  # ëŒ€ëµì  ì¶”ì •


def extract_dictionary_info(filename: str) -> Tuple[str, Optional[int]]:
    """
    íŒŒì¼ëª…ì—ì„œ ì‚¬ì „ ì•½ì–´ì™€ ê¶Œ ë²ˆí˜¸ ì¶”ì¶œ
    
    Examples:
        'ThWAT_Vol1.pdf' -> ('ThWAT', 1)
        'EWNT_Bd2.pdf' -> ('EWNT', 2)
        'HWPh.pdf' -> ('HWPh', None)
    """
    stem = Path(filename).stem
    
    # ì‚¬ì „ ì•½ì–´ ì¶”ì¶œ
    dict_abbrev = None
    for abbrev, full_name in DICTIONARY_ABBREVS.items():
        if abbrev.lower() in stem.lower():
            # "Theologische" ë§¤ì¹­ ì‹œ "TRE"ë¡œ ë³€í™˜
            dict_abbrev = "TRE" if abbrev == "Theologische" else abbrev
            break
    
    if not dict_abbrev:
        dict_abbrev = stem.split('_')[0].split()[0].upper()
    
    # ê¶Œ ë²ˆí˜¸ ì¶”ì¶œ
    volume_patterns = [
        r'(?:Vol|Bd|Band|ê¶Œ|V)[.\-_\s]?(\d+)',
        r'(\d+)(?:\s*ê¶Œ|\s*Bd)',
        r'[,\s](\d+)[\s,]',
        r'\s(\d+)[\s_-]',
        r'_(\d+)$',
    ]
    
    volume = None
    for pattern in volume_patterns:
        match = re.search(pattern, stem, re.IGNORECASE)
        if match:
            volume = int(match.group(1))
            break
    
    return dict_abbrev, volume


def extract_lemma(text: str) -> Optional[str]:
    """
    í…ìŠ¤íŠ¸ ì‹œì‘ ë¶€ë¶„ì—ì„œ í‘œì œì–´(Lemma) ì¶”ì¶œ
    
    ì‹ í•™ ì‚¬ì „ í‘œì œì–´ íŒ¨í„´:
    - ì „ì²´ ëŒ€ë¬¸ì: "GNADE", "ABRAHAM"
    - ë…ì¼ì–´: "Alkohol und Alkoholismus"
    - ê·¸ë¦¬ìŠ¤/íˆë¸Œë¦¬ì–´: "á¼€Î³Î¬Ï€Î·", "×—Ö¶×¡Ö¶×“"
    """
    sample = text.strip()[:300]
    
    patterns = [
        # ì „ì²´ ëŒ€ë¬¸ì (TDNT ìŠ¤íƒ€ì¼)
        r'^([A-ZÃ„Ã–Ãœ]{2,}(?:\s+[A-ZÃ„Ã–Ãœ]{2,})*)[\.,\s]',
        
        # ë…ì¼ì–´/ì˜ì–´ (HWPh, ThWAT ìŠ¤íƒ€ì¼)
        r'^([A-ZÃ„Ã–Ãœ][a-zÃ¤Ã¶Ã¼ÃŸ]+(?:\s+(?:und|u\.|,|/)\s*[A-ZÃ„Ã–Ãœ]?[a-zÃ¤Ã¶Ã¼ÃŸ]+)*)[\.,]',
        
        # ê·¸ë¦¬ìŠ¤ì–´
        r'^([Î±-Ï‰Î¬-ÏÎ‘-Î©]+)[\s,\.]',
        
        # íˆë¸Œë¦¬ì–´
        r'^([×-×ª]+)[\s,\.]',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, sample)
        if match:
            lemma = match.group(1).strip()
            if 2 <= len(lemma) <= 50:
                return lemma
    
    return None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PDF ì²˜ë¦¬
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def process_file(file_path: str, text_splitter, page_offset: int = 0, double_page: bool = False) -> List[Dict[str, Any]]:
    """
    íŒŒì¼(PDF ë˜ëŠ” TXT)ì„ ì²˜ë¦¬í•˜ì—¬ ë©”íƒ€ë°ì´í„°ê°€ í¬í•¨ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    """
    path = Path(file_path)
    if path.suffix.lower() == '.pdf':
        return _process_pdf_content(file_path, text_splitter, page_offset, double_page)
    elif path.suffix.lower() == '.txt':
        return _process_text_content(file_path, text_splitter, page_offset)
    else:
        print(f"      âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {path.suffix}")
        return []

def _process_text_content(txt_path: str, text_splitter, page_offset: int) -> List[Dict[str, Any]]:
    """TXT íŒŒì¼ ì²˜ë¦¬ (í˜ì´ì§€ êµ¬ë¶„ ì—†ìŒ ë˜ëŠ” ê°„ë‹¨í•œ êµ¬ë¶„)"""
    try:
        with open(txt_path, 'r', encoding='utf-8') as f:
            text = f.read()
    except Exception as e:
        print(f"      âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
        return []

    # í…ìŠ¤íŠ¸ ì²­í‚¹
    chunks = text_splitter.split_text(text)
    
    # ë©”íƒ€ë°ì´í„° ìƒì„± (TXTëŠ” í˜ì´ì§€ ê°œë…ì´ ëª¨í˜¸í•˜ë¯€ë¡œ ì „ì²´ 1í˜ì´ì§€ë¡œ ê°€ì •í•˜ê±°ë‚˜ ë³„ë„ ë¡œì§ í•„ìš”)
    filename = Path(txt_path).name
    dict_abbrev, volume = extract_dictionary_info(filename)
    
    result = []
    chunk_lemmas = []
    
    # 1. í‘œì œì–´ ê°ì§€
    current_lemma = None
    for chunk in chunks:
        detected = extract_lemma(chunk)
        if detected:
            current_lemma = detected
        chunk_lemmas.append(current_lemma)
        
    # 2. í‘œì œì–´ ì¹´ìš´íŠ¸
    lemma_chunk_counts = defaultdict(int)
    for lemma in chunk_lemmas:
        if lemma: lemma_chunk_counts[lemma] += 1
        
    lemma_current_index = defaultdict(int)
    
    for i, chunk in enumerate(chunks):
        lemma = chunk_lemmas[i]
        if lemma:
            lemma_current_index[lemma] += 1
            lemma_idx = lemma_current_index[lemma]
            lemma_total = lemma_chunk_counts[lemma]
        else:
            lemma_idx = None
            lemma_total = None
            
        metadata = {
            "source": dict_abbrev,
            "filename": filename,
            "chunk_id": f"chunk_{i}",
            "page_number": 1, # TXTëŠ” ê¸°ë³¸ 1
            "pdf_page": 1,
            "total_pages": 1,
            "chunk_tokens": tiktoken_len(chunk),
            "volume": volume,
            "lemma": lemma,
            "lemma_chunk_index": lemma_idx,
            "lemma_total_chunks": lemma_total,
            "is_spread": False
        }
        
        chunk_id = f"{dict_abbrev}"
        if volume: chunk_id += f"_{volume}"
        if lemma: chunk_id += f"_{lemma[:20]}"
        chunk_id += f"_{i:04d}"
        
        result.append({
            "id": chunk_id,
            "text": chunk,
            "metadata": metadata
        })
        
    print(f"      âœ… {len(result):,}ê°œ í…ìŠ¤íŠ¸ ì²­í¬ ìƒì„±")
    return result

def _process_pdf_content(pdf_path: str, text_splitter, page_offset: int = 0, double_page: bool = False) -> List[Dict[str, Any]]:
    """
    PDF ë‚´ìš©ì„ ì²˜ë¦¬í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜ (ê¸°ì¡´ process_pdf ë¡œì§)
    """
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ğŸ“– Double Page / Layout ì²˜ë¦¬ (TRE Bd.4 ì „ìš© ë§¤í•‘ í¬í•¨)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def get_tre_bd4_pages(pdf_page: int) -> Tuple[Optional[int], Optional[int]]:
        """TRE Bd.4 ì „ìš© ë¶ˆê·œì¹™ ë§¤í•‘ ë¡œì§"""
        if pdf_page < 2: return None, None
        if pdf_page == 2: return None, 1
        if 3 <= pdf_page <= 88:
            return (pdf_page - 2) * 2, (pdf_page - 2) * 2 + 1
        if pdf_page == 89: return 174, None  # Right is blank
        if pdf_page == 90: return None, 175  # Left is blank
        if 91 <= pdf_page <= 266:
            return (pdf_page - 3) * 2, (pdf_page - 3) * 2 + 1
        if pdf_page == 267: return 528, None  # Right is Tafel
        if pdf_page == 268: return None, 529  # Left is Tafel
        if pdf_page >= 269:
            return (pdf_page - 4) * 2, (pdf_page - 4) * 2 + 1
        return None, None

    def get_tre_bd5_pages(pdf_page: int) -> Tuple[Optional[int], Optional[int]]:
        """TRE Bd.5 ì „ìš© ë¶ˆê·œì¹™ ë§¤í•‘ ë¡œì§"""
        if pdf_page < 2: return None, None
        if pdf_page == 2: return None, 1
        if 3 <= pdf_page <= 125:
            return (pdf_page - 2) * 2, (pdf_page - 2) * 2 + 1
        if pdf_page == 126: return 248, None
        if pdf_page == 127: return None, None
        if pdf_page == 128: return None, 249
        if 129 <= pdf_page <= 259:
            return (pdf_page - 4) * 2, (pdf_page - 4) * 2 + 1
        if pdf_page == 260: return 512, None
        if 261 <= pdf_page <= 262: return None, None
        if pdf_page == 263: return None, 513
        if pdf_page >= 264:
            return (pdf_page - 7) * 2, (pdf_page - 7) * 2 + 1
        return None, None

    def get_kd_ii2_page(pdf_page: int) -> Optional[int]:
        """KD II/2 ì „ìš© êµ¬ê°„ë³„ ì˜¤í”„ì…‹ ë§¤í•‘"""
        if pdf_page < 10: return None
        if 10 <= pdf_page <= 716:
            return pdf_page - 9
        if 717 <= pdf_page <= 874:
            return pdf_page - 10
        if 875 <= pdf_page <= 885:
            return pdf_page - 12
        if pdf_page >= 886:
            return pdf_page - 11
        return None

    def get_kd_iv4_page(pdf_page: int) -> Optional[int]:
        """KD IV/4 ì „ìš© ì˜¤í”„ì…‹ ë§¤í•‘ (PDF 13 -> ì¢…ì´ 2, ì˜¤í”„ì…‹ 11)"""
        if pdf_page < 13: return None
        return pdf_page - 11

    def split_double_page(page) -> Tuple[str, str]:
        """PDF í˜ì´ì§€ë¥¼ ì¢Œ/ìš° í…ìŠ¤íŠ¸ë¡œ ë¶„í•  (ì¢Œí‘œ ê¸°ë°˜)"""
        left_parts = []
        right_parts = []
        middle = float(page.mediabox.width) / 2
        
        # í…ìŠ¤íŠ¸ ì¡°ê°ê³¼ ì¢Œí‘œ ìˆ˜ì§‘
        parts = []
        def visitor(text, cm, tm, font_dict, font_size):
            if text.strip():
                parts.append({
                    'text': text,
                    'x': tm[4],
                    'y': tm[5]
                })
        
        page.extract_text(visitor_text=visitor)
        
        # Yì¢Œí‘œ(ìœ„->ì•„ë˜) ìˆœìœ¼ë¡œ ì •ë ¬ (pypdf ì¶”ì¶œ ìˆœì„œê°€ ê¼¬ì¼ ìˆ˜ ìˆìŒ)
        parts.sort(key=lambda p: (-p['y'], p['x']))
        
        for p in parts:
            if p['x'] < middle:
                left_parts.append(p['text'])
            else:
                right_parts.append(p['text'])
                
        return "".join(left_parts), "".join(right_parts)

    filename = Path(pdf_path).name
    dict_abbrev, volume = extract_dictionary_info(filename)
    is_tre_bd4 = (dict_abbrev == "TRE" and volume == 4)
    is_tre_bd5 = (dict_abbrev == "TRE" and volume == 5)
    stem_lower = Path(filename).stem.lower()
    is_kd_ii2 = ("kd" in stem_lower and ("ii-2" in stem_lower or "ii.2" in stem_lower or "ii_2" in stem_lower))
    is_kd_iv4 = ("kd" in stem_lower and ("iv-4" in stem_lower or "iv.4" in stem_lower or "iv_4" in stem_lower))
    
    print(f"   ğŸ“– {filename}")
    print(f"      ì‚¬ì „: {dict_abbrev}, ê¶Œ: {volume or 'ë‹¨ê¶Œ'}")
    if double_page:
        mode_str = "TRE Bd.4 íŠ¹ìˆ˜ ë§¤í•‘" if is_tre_bd4 else ("TRE Bd.5 íŠ¹ìˆ˜ ë§¤í•‘" if is_tre_bd5 else "ì¼ë°˜ ìŠ¤í”„ë ˆë“œ")
        print(f"      âœ¨ ìŠ¤í”„ë ˆë“œ ëª¨ë“œ í™œì„±í™” ({mode_str})")
    elif is_kd_ii2:
        print(f"      âœ¨ KD II.2 íŠ¹ìˆ˜ êµ¬ê°„ ë§¤í•‘ í™œì„±í™”")
    elif is_kd_iv4:
        print(f"      âœ¨ KD IV/4 ê¸°ë³¸ ì˜¤í”„ì…‹(-11) ë§¤í•‘ í™œì„±í™”")
    
    # PDF ì½ê¸°
    reader = PdfReader(pdf_path)
    
    # í˜ì´ì§€ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
    pages_data = []
    
    for page_num, page in enumerate(reader.pages, 1):
        if double_page:
            left_txt, right_txt = split_double_page(page)
            
            # í˜ì´ì§€ ë²ˆí˜¸ ê²°ì •
            if is_tre_bd4:
                lp, rp = get_tre_bd4_pages(page_num)
            elif is_tre_bd5:
                lp, rp = get_tre_bd5_pages(page_num)
            else:
                # ê¸°ë³¸ ìŠ¤í”„ë ˆë“œ ê³µì‹
                lp = (page_num - page_offset) * 2 - 1
                rp = (page_num - page_offset) * 2
            
            if left_txt.strip() and lp:
                pages_data.append({
                    'page_num': lp,
                    'pdf_page': page_num,
                    'text': left_txt,
                    'char_start': sum(len(p['text']) for p in pages_data)
                })
            if right_txt.strip() and rp:
                pages_data.append({
                    'page_num': rp,
                    'pdf_page': page_num,
                    'text': right_txt,
                    'char_start': sum(len(p['text']) for p in pages_data)
                })
        else:
            text = page.extract_text() or ""
            if text.strip():
                # KD II.2 íŠ¹ìˆ˜ ë§¤í•‘ ë˜ëŠ” ì¼ë°˜ ì˜¤í”„ì…‹ ë§¤í•‘ ì ìš©
                if is_kd_ii2:
                    p_num = get_kd_ii2_page(page_num)
                elif is_kd_iv4:
                    p_num = get_kd_iv4_page(page_num)
                else:
                    p_num = max(1, page_num - page_offset)
                
                if p_num:
                    pages_data.append({
                        'page_num': p_num,
                        'pdf_page': page_num,
                        'text': text,
                        'char_start': sum(len(p['text']) for p in pages_data)
                    })
    
    if not pages_data:
        print(f"      âš ï¸  ë¹ˆ PDF")
        return []
    
    # í…ìŠ¤íŠ¸ í†µí•© ë° ì²­í‚¹
    full_text = "\n\n".join(p['text'] for p in pages_data)
    chunks = text_splitter.split_text(full_text)
    
    # í‘œì œì–´ ê°ì§€ (1ì°¨ íŒ¨ìŠ¤)
    chunk_lemmas = []
    current_lemma = None
    for chunk in chunks:
        detected = extract_lemma(chunk)
        if detected:
            current_lemma = detected
        chunk_lemmas.append(current_lemma)
    
    # í‘œì œì–´ë³„ ì²­í¬ ìˆ˜ ê³„ì‚° (2ì°¨ íŒ¨ìŠ¤)
    lemma_chunk_counts = defaultdict(int)
    for lemma in chunk_lemmas:
        if lemma:
            lemma_chunk_counts[lemma] += 1
    
    # ì²­í¬ ë©”íƒ€ë°ì´í„° ìƒì„± (3ì°¨ íŒ¨ìŠ¤)
    result = []
    char_position = 0
    lemma_current_index = defaultdict(int)
    
    for i, chunk in enumerate(chunks):
        chunk_mid = char_position + len(chunk) // 2
        
        # í˜ì´ì§€ ë²ˆí˜¸ ë° ì›ë³¸ PDF í˜ì´ì§€ ì°¾ê¸°
        page_num = 1
        pdf_page = 1
        for p in pages_data:
            if chunk_mid >= p['char_start']:
                page_num = p['page_num']
                pdf_page = p.get('pdf_page', page_num)
            else:
                break
        
        # í‘œì œì–´ ì •ë³´
        lemma = chunk_lemmas[i]
        if lemma:
            lemma_current_index[lemma] += 1
            lemma_idx = lemma_current_index[lemma]
            lemma_total = lemma_chunk_counts[lemma]
        else:
            lemma_idx = None
            lemma_total = None
        
        # ë©”íƒ€ë°ì´í„° (ì´ë¯¸ ë³´ì •ëœ paper_page ì‚¬ìš©)
        metadata = {
            "source": dict_abbrev,
            "filename": filename,
            "chunk_id": f"chunk_{i}",
            "page_number": page_num,
            "pdf_page": pdf_page,
            "total_pages": len(reader.pages),
            "chunk_tokens": tiktoken_len(chunk),
            "volume": volume,
            "lemma": lemma,
            "lemma_chunk_index": lemma_idx,
            "lemma_total_chunks": lemma_total,
            "is_spread": double_page
        }
        
        # ID ìƒì„±
        chunk_id = f"{dict_abbrev}"
        if volume:
            chunk_id += f"_{volume}"
        if lemma:
            chunk_id += f"_{lemma[:20]}"
        chunk_id += f"_{i:04d}"
        
        result.append({
            "id": chunk_id,
            "text": chunk,
            "metadata": metadata
        })
        
        char_position += len(chunk)
    
    # í†µê³„
    unique_lemmas = len([l for l in set(chunk_lemmas) if l])
    print(f"      âœ… {len(result):,}ê°œ ì²­í¬ ìƒì„± ({unique_lemmas}ê°œ í‘œì œì–´ ê°ì§€)")
    
    return result


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë©”ì¸ ì‹¤í–‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description='PDFë¥¼ ChromaDBìš© JSONìœ¼ë¡œ ë³€í™˜'
    )
    parser.add_argument('input', help='PDF íŒŒì¼ ë˜ëŠ” í´ë” ê²½ë¡œ')
    parser.add_argument('-o', '--output', default=os.path.expanduser("~/Desktop/MS_Brain.nosync/300 Tech/320 Coding/Projects.nosync/Theology_Project.nosync/inbox"),
                        help='ì¶œë ¥ í´ë” (ê¸°ë³¸: inbox)')
    parser.add_argument('--chunk-size', type=int, default=2800,
                        help='ì²­í¬ í¬ê¸° (ë¬¸ì ìˆ˜)')
    parser.add_argument('--overlap', type=int, default=560,
                        help='ì²­í¬ ì˜¤ë²„ë© (ë¬¸ì ìˆ˜)')
    parser.add_argument('--page-offset', type=int, default=0,
                        help='PDFí˜ì´ì§€ - ì˜¤í”„ì…‹ = ì¢…ì´ì±… í˜ì´ì§€ (ì˜ˆ: TREëŠ” 2)')
    parser.add_argument('--double-page', action='store_true',
                        help='PDF 1í˜ì´ì§€ì— ì¢…ì´ì±… 2í˜ì´ì§€ê°€ í¬í•¨ëœ ê²½ìš°(ìŠ¤í”„ë ˆë“œ)')
    
    args = parser.parse_args()
    
    # ì¶œë ¥ í´ë” ìƒì„±
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # í…ìŠ¤íŠ¸ ìŠ¤í”Œë¦¬í„° ì„¤ì •
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=args.chunk_size,
        chunk_overlap=args.overlap,
        separators=["\n\n\n", "\n\n", "\n", ". ", "! ", "? ", "; ", " ", ""],
        length_function=tiktoken_len,
        is_separator_regex=False,
    )
    
    print("=" * 60)
    print("ğŸ“š ë¡œì»¬ PDF â†’ ChromaDB JSON ë³€í™˜")
    print("=" * 60)
    print(f"\nì…ë ¥: {args.input}")
    print(f"ì¶œë ¥: {output_dir}")
    print(f"ì²­í‚¹: ~{args.chunk_size//4} í† í° (ì˜¤ë²„ë© ~{args.overlap//4})\n")
    
    # ì…ë ¥ ê²½ë¡œ í™•ì¸
    input_path = Path(args.input)
    
    if input_path.is_file():
        input_files = [input_path]
    elif input_path.is_dir():
        input_files = list(input_path.glob("*.pdf")) + list(input_path.glob("*.txt"))
    else:
        print(f"âŒ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.input}")
        return
    
    if not input_files:
        print("âŒ ì²˜ë¦¬í•  íŒŒì¼(PDF/TXT)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“‚ ë°œê²¬ëœ íŒŒì¼: {len(input_files)}ê°œ\n")
    
    # PDF ì²˜ë¦¬
    all_chunks_by_source = defaultdict(list)
    total_chunks = 0
    
    for i, input_file in enumerate(input_files, 1):
        print(f"[{i}/{len(input_files)}]")
        try:
            chunks = process_file(str(input_file), text_splitter, args.page_offset, args.double_page)
            
            if chunks:
                # ì†ŒìŠ¤ë³„ ê·¸ë£¹í™”
                source = chunks[0]['metadata']['source']
                volume = chunks[0]['metadata'].get('volume')
                
                if volume:
                    source_key = f"{source}_Bd{volume}"
                else:
                    source_key = source
                
                all_chunks_by_source[source_key].extend(chunks)
                total_chunks += len(chunks)
        
        except Exception as e:
            print(f"      âŒ ì˜¤ë¥˜: {e}")
        
        print()
    
    # JSON ì €ì¥
    print("=" * 60)
    print(f"ğŸ’¾ JSON ì €ì¥ ì¤‘... (ì´ {total_chunks:,}ê°œ ì²­í¬)")
    print("=" * 60)
    print()
    
    for source_key, chunks in all_chunks_by_source.items():
        output_file = output_dir / f"{source_key}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            for chunk in chunks:
                # None ê°’ ì œê±°
                clean_meta = {
                    k: (v if v is not None else "")
                    for k, v in chunk['metadata'].items()
                }
                chunk['metadata'] = clean_meta
                
                f.write(json.dumps(chunk, ensure_ascii=False) + '\n')
        
        file_size = output_file.stat().st_size / (1024 * 1024)
        print(f"âœ… {source_key}.json")
        print(f"   {len(chunks):,}ê°œ ì²­í¬, {file_size:.1f}MB")
    
    print("\n" + "=" * 60)
    print("ğŸ‰ ì™„ë£Œ!")
    print("=" * 60)
    print(f"\në‹¤ìŒ ë‹¨ê³„:")
    print(f"  cd /Users/msn/Desktop/project/theology-vector-db")
    print(f"  source venv.nosync/bin/activate")
    print(f"  python db_builder.py")


if __name__ == "__main__":
    main()
