#!/usr/bin/env python3
"""
ì‹ í•™ DB ê²€ìƒ‰ CLI (v4 - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰)
Usage: python search.py --query "ê²€ìƒ‰ì–´" [--source RGG] [--n 5]

Features:
- ì˜ë¯¸ ê²€ìƒ‰ (Vector) + í‚¤ì›Œë“œ ê²€ìƒ‰ (BM25) í•˜ì´ë¸Œë¦¬ë“œ
- ê³ ì „ì–´ (íˆë¸Œë¦¬ì–´/í—¬ë¼ì–´) ë° ìŒì—­ì–´ ê²€ìƒ‰ ì§€ì›
- ê¶Œ(Volume) ë° í‘œì œì–´(Lemma) í‘œì‹œ
- í•™ìˆ  ì¸ìš© í˜•ì‹ ì¶œë ¥ (ì˜ˆ: TDNT I, p.35)
"""

import sys
import os
import argparse
import json
import glob
from pathlib import Path
from typing import List, Dict, Any, Tuple

import chromadb
from sentence_transformers import SentenceTransformer

# BM25 (optional, graceful fallback)
try:
    from rank_bm25 import BM25Okapi
    BM25_AVAILABLE = True
except ImportError:
    BM25_AVAILABLE = False


def discover_paths() -> Tuple[str, str]:
    """DB ë° Archive ê²½ë¡œ íƒì§€"""
    script_dir = Path(__file__).parent
    
    # Kit êµ¬ì¡° (ë°°í¬ìš©)
    kit_root = script_dir.parent.parent
    kit_db = kit_root / "02_Brain" / "vector_db"
    kit_archive = kit_root / "01_Library" / "archive"
    
    if kit_db.exists():
        return str(kit_db), str(kit_archive)
    
    # ê°œë°œ êµ¬ì¡°
    dev_db = os.path.expanduser("~/Desktop/MS_Dev.nosync/data/Theology_Project.nosync/vector_db")
    dev_archive = os.path.expanduser("~/Desktop/MS_Dev.nosync/data/Theology_Project.nosync/archive")
    
    return dev_db, dev_archive


DB_PATH, ARCHIVE_PATH = discover_paths()

ROMAN_NUMERALS = {1: 'I', 2: 'II', 3: 'III', 4: 'IV', 5: 'V', 
                  6: 'VI', 7: 'VII', 8: 'VIII', 9: 'IX', 10: 'X',
                  11: 'XI', 12: 'XII', 13: 'XIII', 14: 'XIV', 15: 'XV'}


def format_citation(meta: dict) -> str:
    """í•™ìˆ  ì¸ìš© í˜•ì‹ìœ¼ë¡œ ì¶œë ¥"""
    source = meta.get('source', 'Unknown')
    if '_Vol' in source:
        source = source.split('_Vol')[0]
    
    volume = meta.get('volume')
    page = meta.get('page_number', '?')
    lemma = meta.get('lemma')
    
    citation = source
    if volume and volume != "":
        vol_roman = ROMAN_NUMERALS.get(int(volume), str(volume)) if isinstance(volume, (int, str)) and str(volume).isdigit() else volume
        citation += f" {vol_roman}"
    if page and page != "N/A":
        citation += f", p.{page}"
    if lemma and lemma != "":
        citation += f" â€“ {lemma}"
    return citation


def load_bm25_corpus() -> Tuple[List[str], List[Dict], 'BM25Okapi']:
    """Archive JSONì—ì„œ BM25 ì½”í¼ìŠ¤ ë¡œë“œ"""
    if not BM25_AVAILABLE:
        return [], [], None
    
    archive_path = Path(ARCHIVE_PATH)
    if not archive_path.exists():
        return [], [], None
    
    documents = []
    metadatas = []
    
    json_files = list(archive_path.glob("*.json"))
    for jf in json_files:
        try:
            with open(jf, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, list):
                    for chunk in data:
                        text = chunk.get('text', chunk.get('content', ''))
                        if text:
                            documents.append(text)
                            metadatas.append(chunk.get('metadata', chunk))
        except Exception:
            continue
    
    if not documents:
        return [], [], None
    
    # í† í°í™” (ê°„ë‹¨íˆ ê³µë°± ë¶„ë¦¬)
    tokenized = [doc.lower().split() for doc in documents]
    bm25 = BM25Okapi(tokenized)
    
    return documents, metadatas, bm25


def bm25_search(query: str, bm25: 'BM25Okapi', documents: List[str], 
                metadatas: List[Dict], n_results: int = 10) -> List[Dict]:
    """BM25 í‚¤ì›Œë“œ ê²€ìƒ‰"""
    if bm25 is None:
        return []
    
    tokenized_query = query.lower().split()
    scores = bm25.get_scores(tokenized_query)
    
    # ìƒìœ„ ê²°ê³¼ ì¶”ì¶œ
    top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:n_results * 2]
    
    results = []
    for idx in top_indices:
        if scores[idx] > 0:  # ì ìˆ˜ê°€ ìˆëŠ” ê²ƒë§Œ
            results.append({
                "text": documents[idx],
                "metadata": metadatas[idx],
                "score": float(scores[idx]),
                "method": "bm25"
            })
    
    return results[:n_results]


def vector_search(query: str, model, collection, n_results: int = 10, 
                  source_filter: str = None) -> List[Dict]:
    """ë²¡í„° ì˜ë¯¸ ê²€ìƒ‰"""
    query_vec = model.encode([query]).tolist()
    
    fetch_n = n_results * 10 if source_filter else n_results * 2
    results = collection.query(query_embeddings=query_vec, n_results=fetch_n)
    
    output = []
    if results['documents'] and results['documents'][0]:
        for i, doc in enumerate(results['documents'][0]):
            meta = results['metadatas'][0][i]
            if meta is None:
                continue
            
            # ì†ŒìŠ¤ í•„í„°ë§
            if source_filter:
                source_val = meta.get('source', '')
                if source_filter.lower() not in source_val.lower():
                    continue
            
            output.append({
                "text": doc,
                "metadata": meta,
                "score": 1.0 - (i * 0.01),  # ìˆœìœ„ ê¸°ë°˜ ì ìˆ˜
                "method": "vector"
            })
            
            if len(output) >= n_results:
                break
    
    return output


def hybrid_search(query: str, source: str = None, n_results: int = 5) -> List[Dict]:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (TheologySearcher v4 ì‚¬ìš©)"""
    from pipeline.embedder import TheologyEmbedder
    from pipeline.searcher import TheologySearcher
    from langchain_chroma import Chroma
    from langchain_core.documents import Document
    
    # 1. ì»´í¬ë„ŒíŠ¸ ë¡œë“œ
    embedder = TheologyEmbedder()
    
    # Chroma DB ì—°ê²°
    vector_db = Chroma(
        persist_directory=DB_PATH,
        embedding_function=lambda x: embedder.embed_documents(x),
        collection_name="theology_library"
    )
    
    searcher = TheologySearcher(vector_db)
    
    # 2. í•˜ì´ë¸Œë¦¬ë“œ ì¸ë±ìŠ¤ êµ¬ì¶•ì„ ìœ„í•´ ì „ì²´ ë¬¸ì„œ ë¡œë“œ (CLI í™˜ê²½ì´ë¯€ë¡œ ì‹¤ì‹œê°„ êµ¬ì¶•)
    try:
        results = vector_db.get(include=["documents", "metadatas"])
        if results and results["documents"]:
            all_docs = [
                Document(page_content=d, metadata=m) 
                for d, m in zip(results["documents"], results["metadatas"])
            ]
            searcher.build_ensemble(all_docs, k=n_results)
    except Exception as e:
        print(f"âš ï¸ Hybrid Searcher ë¹Œë“œ ì‹¤íŒ¨: {e}")
    
    # 3. ê²€ìƒ‰ ì‹¤í–‰
    raw_results = searcher.search(query)
    
    # 4. ì†ŒìŠ¤ í•„í„°ë§ ë° í¬ë§· ë³€í™˜
    output = []
    for r in raw_results:
        meta = r.metadata
        if source and source.lower() not in meta.get('source', '').lower():
            continue
            
        output.append({
            "text": r.page_content,
            "metadata": meta,
            "score": 1.0,
            "method": "hybrid"
        })
        
        if len(output) >= n_results:
            break
            
    return output


def search(query: str, source: str = None, n_results: int = 5, output_json: bool = False):
    """í†µí•© ê²€ìƒ‰ í•¨ìˆ˜ (í•˜ì´ë¸Œë¦¬ë“œ)"""
    
    results = hybrid_search(query, source, n_results)
    
    output = []
    for i, r in enumerate(results):
        meta = r.get('metadata', {})
        citation = format_citation(meta)
        method_tag = "ğŸ”¤" if r.get('method') == 'bm25' else "ğŸ§ "
        
        lemma_info = ""
        if meta.get('lemma_chunk_index') and meta.get('lemma_total_chunks'):
            lemma_info = f" [{meta['lemma_chunk_index']}/{meta['lemma_total_chunks']}]"
        
        result_item = {
            "rank": i + 1,
            "citation": citation + lemma_info,
            "text": r['text'][:800] if len(r['text']) > 800 else r['text'],
            "metadata": meta,
            "method": r.get('method', 'unknown')
        }
        output.append(result_item)
        
        if not output_json:
            print(f"â”â”â” [{i+1}] {method_tag} {citation}{lemma_info} â”â”â”")
            print(r['text'][:500] + "..." if len(r['text']) > 500 else r['text'])
            print()
    
    if output_json:
        print(json.dumps({
            "query": query, 
            "source_filter": source, 
            "hybrid_mode": BM25_AVAILABLE,
            "results": output
        }, ensure_ascii=False, indent=2))
    elif not results:
        print(f"ì¡°ê±´ì— ë§ëŠ” ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    return output


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ì‹ í•™ DB í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ CLI")
    parser.add_argument("-q", "--query", required=True, help="ê²€ìƒ‰ì–´ (í•œê¸€, ë…ì¼ì–´, íˆë¸Œë¦¬ì–´, í—¬ë¼ì–´, ìŒì—­ ê°€ëŠ¥)")
    parser.add_argument("-s", "--source", help="ì†ŒìŠ¤ í•„í„° (ì˜ˆ: RGG, EKL, TDNT)")
    parser.add_argument("-n", "--num", type=int, default=5, help="ê²°ê³¼ ìˆ˜ (ê¸°ë³¸: 5)")
    parser.add_argument("--json", action="store_true", help="JSON í˜•ì‹ ì¶œë ¥")
    
    args = parser.parse_args()
    
    if not args.json:
        print(f"ğŸ” ê²€ìƒ‰ì–´: {args.query}")
        if args.source:
            print(f"ğŸ“š ì†ŒìŠ¤ í•„í„°: {args.source}")
        print(f"ğŸ”„ ê²€ìƒ‰ ëª¨ë“œ: {'í•˜ì´ë¸Œë¦¬ë“œ (Vector + BM25)' if BM25_AVAILABLE else 'ë²¡í„° ì „ìš©'}")
        print()
    
    search(args.query, args.source, args.num, args.json)
