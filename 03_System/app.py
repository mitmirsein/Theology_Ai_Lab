#!/usr/bin/env python3
"""
Theology AI Lab - Streamlit GUI
================================
ë¡œì»¬ ì‹ í•™ ì—°êµ¬ ë°ì´í„°ë² ì´ìŠ¤ ì¸í„°í˜ì´ìŠ¤

ì‹¤í–‰: streamlit run app.py
"""

import os

# tokenizers fork í¬ë˜ì‹œ ë°©ì§€ (ë©€í‹°ìŠ¤ë ˆë“œ í™˜ê²½ì—ì„œ subprocess í˜¸ì¶œ ì‹œ ì¶©ëŒ ë°©ì§€)
os.environ["TOKENIZERS_PARALLELISM"] = "false"
import sys
import json
import shutil
import re
from pathlib import Path
from datetime import datetime

import streamlit as st
import logging

# Logger setup
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("App")

# ê²½ë¡œ ì„¤ì •
SCRIPT_DIR = Path(__file__).parent.absolute()
KIT_ROOT = SCRIPT_DIR.parent

sys.path.insert(0, str(SCRIPT_DIR))
sys.path.insert(0, str(SCRIPT_DIR / "utils"))

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv
ENV_FILE = KIT_ROOT / ".env"
load_dotenv(ENV_FILE)

# ì „ì—­ ì„¤ì • ë¡œë“œ
def load_global_settings():
    settings = {
        "ANTHROPIC_API_KEY": os.getenv("ANTHROPIC_API_KEY", ""),
        "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY", ""),
        "GOOGLE_API_KEY": os.getenv("GOOGLE_API_KEY", ""),
        "RAG_MODEL": os.getenv("RAG_MODEL", "gemini-2.5-flash"),
        "RAG_MAX_TOKENS": os.getenv("RAG_MAX_TOKENS", "4096"),
        "APP_TITLE": os.getenv("APP_TITLE", "Kerygma Th Library"),
        "OBSIDIAN_VAULT": os.getenv("OBSIDIAN_VAULT", ""),
    }
    # .env íŒŒì¼ì´ ìˆìœ¼ë©´ ìš°ì„ ì ìœ¼ë¡œ ë®ì–´ì“°ê¸° (ì‹¤ì‹œê°„ ë°˜ì˜ìš©)
    if ENV_FILE.exists():
        with open(ENV_FILE, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith("#") and "=" in line:
                    key, value = line.split("=", 1)
                    key = key.strip()
                    value = value.strip().strip('"').strip("'")
                    if key in settings:
                        settings[key] = value
    return settings

GLOBAL_SETTINGS = load_global_settings()
APP_TITLE = GLOBAL_SETTINGS["APP_TITLE"]

# ê²½ë¡œ ì„¤ì • (ìƒëŒ€ ê²½ë¡œëŠ” KIT_ROOT ê¸°ì¤€ìœ¼ë¡œ ë³€í™˜)
def resolve_path(env_var: str, default: str) -> Path:
    """í™˜ê²½ ë³€ìˆ˜ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜"""
    val = os.getenv(env_var, default)
    if val is None:
        val = default
    if val.startswith("."):
        return (KIT_ROOT / val).resolve()
    return Path(val)

INBOX_DIR = resolve_path("INBOX_DIR", "./01_Library/inbox")
ARCHIVE_DIR = resolve_path("ARCHIVE_DIR", "./01_Library/archive")
DB_PATH = resolve_path("CHROMA_DB_DIR", "./02_Brain/vector_db")
LEMMA_INDEX_PATH = ARCHIVE_DIR / "lemma_index.json"

# ============================================================
# í˜ì´ì§€ ì„¤ì •
# ============================================================
st.set_page_config(
    page_title=APP_TITLE,
    page_icon="ğŸ“œ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ============================================================
# ìºì‹œëœ ë¦¬ì†ŒìŠ¤ ë¡œë“œ
# ============================================================
@st.cache_resource
def load_embedder():
    """BGE-M3 ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (M1 ê°€ì† ì§€ì›)"""
    from pipeline.embedder import TheologyEmbedder
    return TheologyEmbedder()

@st.cache_resource
def load_searcher(_db_path: str):
    """Hybrid ê²€ìƒ‰ ì—”ì§„ ë¡œë“œ"""
    from pipeline.searcher import TheologySearcher
    from langchain_chroma import Chroma
    from langchain_core.documents import Document
    
    # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
    embedder = load_embedder()
    
    # Chroma DB ì—°ê²° (embedder ê°ì²´ ì§ì ‘ ì „ë‹¬)
    vector_db = Chroma(
        persist_directory=_db_path,
        embedding_function=embedder,
        collection_name="theology_library"
    )
    
    searcher = TheologySearcher(vector_db)
    
    # BM25 êµ¬ì„±ì„ ìœ„í•´ ì „ì²´ ë¬¸ì„œ ë¡œë“œ (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìš©)
    try:
        results = vector_db.get(include=["documents", "metadatas"])
        if results and results["documents"]:
            all_docs = [
                Document(page_content=d, metadata=m) 
                for d, m in zip(results["documents"], results["metadatas"])
            ]
            searcher.build_ensemble(all_docs)
    except Exception as e:
        logger.error(f"Failed to build ensemble: {e}")
        
    return searcher

@st.cache_resource
def load_query_expander():
    """3ì¤‘ ì–¸ì–´ ì¿¼ë¦¬ í™•ì¥ê¸° ë¡œë“œ"""
    try:
        from utils.query_expander import QueryExpander
        return QueryExpander()
    except ImportError:
        return None

@st.cache_resource
def load_db(_db_path: str):
    """ChromaDB ì—°ê²°"""
    import chromadb
    db_path = Path(_db_path)
    if not db_path.exists():
        return None, None
    
    try:
        client = chromadb.PersistentClient(path=_db_path)
        collection = client.get_collection(name="theology_library")
        # [v4.0.1] DB ë¬´ê²°ì„± í™•ì¸ (Reset ì§í›„ í…Œì´ë¸” ì—†ìŒ ì—ëŸ¬ ë°©ì§€)
        _ = collection.count() 
        return client, collection
    except Exception:
        return None, None

@st.cache_data(ttl=60)
def load_lemma_index(_index_path: str):
    """Lemma ì¸ë±ìŠ¤ ë¡œë“œ (v1.0 ë° v2.0 í˜•ì‹ ëª¨ë‘ ì§€ì›)"""
    index_path = Path(_index_path)
    if not index_path.exists():
        return None
    with open(index_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    # v2.0 í˜•ì‹ì¸ì§€ í™•ì¸ (entries í‚¤ê°€ ìˆëŠ”ì§€)
    if "entries" in data:
        return data

    # v1.0 í˜•ì‹ â†’ v2.0 í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    # v1.0: {"lemma": [{file, page}, ...], ...}
    # v2.0: {"entries": {...}, "by_category": {}, "by_source": {...}, "updated_at": ...}
    entries = {}
    by_source = {}

    for lemma, occurrences in data.items():
        entries[lemma] = []
        for occ in occurrences:
            source_file = occ.get("file", "")
            source_name = source_file.replace(".json", "") if source_file else "Unknown"
            page = occ.get("page", 0)

            entries[lemma].append({
                "file": source_file,
                "page": page,
                "source": source_name
            })

            # by_source ì§‘ê³„
            if source_name not in by_source:
                by_source[source_name] = {"count": 0, "volumes": []}
            by_source[source_name]["count"] += 1

    return {
        "version": "1.0 (converted)",
        "updated_at": datetime.now().isoformat(),
        "entries": entries,
        "by_category": {},  # v1.0ì—ëŠ” ì¹´í…Œê³ ë¦¬ ì •ë³´ ì—†ìŒ
        "by_source": by_source
    }


def get_sources_from_db() -> dict:
    """
    ChromaDBì—ì„œ ì§ì ‘ ì†ŒìŠ¤ ëª©ë¡ê³¼ ì²­í¬ ìˆ˜ ì¡°íšŒ

    Returns:
        {"source_name": {"count": int}, ...}
    """
    import chromadb
    from collections import defaultdict

    try:
        client = chromadb.PersistentClient(path=str(DB_PATH))
        collection = client.get_collection(name="theology_library")
        _ = collection.count() # Health check
    except Exception:
        return {}

    # ëª¨ë“  ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„° ì¡°íšŒ
    results = collection.get(include=["metadatas"])

    if not results.get("metadatas"):
        return {}

    # ì†ŒìŠ¤ë³„ ì§‘ê³„
    source_counts = defaultdict(int)
    for meta in results["metadatas"]:
        source = meta.get("source", "Unknown")
        source_counts[source] += 1

    return {source: {"count": count} for source, count in source_counts.items()}


def delete_source_from_db(source_name: str) -> int:
    """
    ChromaDBì—ì„œ íŠ¹ì • ì†ŒìŠ¤ì˜ ëª¨ë“  ì²­í¬ ì‚­ì œ

    Args:
        source_name: ì‚­ì œí•  ì†ŒìŠ¤ ì´ë¦„ (ì˜ˆ: "TRE_Bd04")

    Returns:
        ì‚­ì œëœ ì²­í¬ ìˆ˜
    """
    import chromadb

    try:
        client = chromadb.PersistentClient(path=str(DB_PATH))
        collection = client.get_collection(name="theology_library")
        _ = collection.count() # Health check
    except Exception:
        return 0

    # í•´ë‹¹ ì†ŒìŠ¤ì˜ ëª¨ë“  ë¬¸ì„œ ID ì¡°íšŒ
    results = collection.get(
        where={"source": source_name},
        include=[]
    )

    ids_to_delete = results.get("ids", [])
    if not ids_to_delete:
        return 0

    # ì‚­ì œ ì‹¤í–‰
    collection.delete(ids=ids_to_delete)

    # ìºì‹œ ë¬´íš¨í™”
    load_db.clear()
    load_lemma_index.clear()

    return len(ids_to_delete)


def reindex_source(source_name: str) -> str:
    """
    ì†ŒìŠ¤ ì¬ì¸ë±ì‹±: DBì—ì„œ ì‚­ì œ í›„ ì•„ì¹´ì´ë¸Œì—ì„œ ë‹¤ì‹œ ì¸ë±ì‹±

    Args:
        source_name: ì¬ì¸ë±ì‹±í•  ì†ŒìŠ¤ ì´ë¦„

    Returns:
        ê²°ê³¼ ë©”ì‹œì§€
    """
    import chromadb
    from sentence_transformers import SentenceTransformer

    # 1. ì•„ì¹´ì´ë¸Œ íŒŒì¼ í™•ì¸
    archive_file = ARCHIVE_DIR / f"{source_name}.json"
    if not archive_file.exists():
        raise FileNotFoundError(f"ì•„ì¹´ì´ë¸Œ íŒŒì¼ ì—†ìŒ: {archive_file}")

    # 2. ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
    deleted_count = delete_source_from_db(source_name)

    # 3. ì•„ì¹´ì´ë¸Œì—ì„œ ë°ì´í„° ë¡œë“œ
    with open(archive_file, "r", encoding="utf-8") as f:
        first_char = f.read(1)
        f.seek(0)

        if first_char == "[":
            data = json.load(f)
        else:
            raw_data = json.load(f)
            if isinstance(raw_data, dict) and "chunks" in raw_data:
                data = raw_data["chunks"]
            elif isinstance(raw_data, dict):
                data = [raw_data]
            else:
                data = [raw_data]

    if not data:
        return f"ì‚­ì œ {deleted_count}ê°œ, ë°ì´í„° ì—†ìŒ"

    # 4. ëª¨ë¸ ë° DB ì—°ê²°
    model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
    try:
        client = chromadb.PersistentClient(path=str(DB_PATH))
        collection = client.get_or_create_collection(name="theology_library")
    except Exception as e:
        return f"DB ì—°ê²° ì‹¤íŒ¨: {e}"

    # 5. ì¬ì¸ë±ì‹±
    documents = []
    ids = []
    metadatas = []

    for item in data:
        unique_id = f"{source_name}_{item.get('id', hash(item.get('text', '')))}"
        meta = item.get("metadata", {})
        meta["source"] = source_name
        meta["indexed_at"] = datetime.now().isoformat()
        meta["reindexed"] = True  # ì¬ì¸ë±ì‹± í‘œì‹œ

        # None ë° ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬
        for k, v in list(meta.items()):
            if v is None:
                meta[k] = ""
            elif isinstance(v, list):
                meta[k] = ", ".join(str(x) for x in v)

        documents.append(item["text"])
        ids.append(unique_id)
        metadatas.append(meta)

    # ë°°ì¹˜ ì²˜ë¦¬
    batch_size = 10
    for i in range(0, len(documents), batch_size):
        batch_docs = documents[i:i + batch_size]
        batch_ids = ids[i:i + batch_size]
        batch_meta = metadatas[i:i + batch_size]

        embeddings = model.encode(batch_docs).tolist()
        collection.upsert(
            ids=batch_ids,
            documents=batch_docs,
            embeddings=embeddings,
            metadatas=batch_meta
        )

    # ìºì‹œ ë¬´íš¨í™”
    load_db.clear()
    load_lemma_index.clear()

    return f"ì‚­ì œ {deleted_count}ê°œ â†’ ìƒˆë¡œ ì¸ë±ì‹± {len(documents)}ê°œ"


def check_duplicate_source(source_name: str) -> dict:
    """
    ì¤‘ë³µ ì¸ë±ì‹± ì—¬ë¶€ í™•ì¸

    Returns:
        {"exists": bool, "count": int, "indexed_at": str}
    """
    import chromadb

    try:
        client = chromadb.PersistentClient(path=str(DB_PATH))
        collection = client.get_collection(name="theology_library")
        _ = collection.count() # Health check
    except Exception:
        return {"exists": False, "count": 0, "indexed_at": None}

    results = collection.get(
        where={"source": source_name},
        include=["metadatas"],
        limit=1
    )

    if not results.get("ids"):
        return {"exists": False, "count": 0, "indexed_at": None}

    # ì „ì²´ ê°œìˆ˜ ì¡°íšŒ
    all_results = collection.get(
        where={"source": source_name},
        include=[]
    )

    indexed_at = None
    if results.get("metadatas"):
        indexed_at = results["metadatas"][0].get("indexed_at", "")

    return {
        "exists": True,
        "count": len(all_results.get("ids", [])),
        "indexed_at": indexed_at
    }


# ============================================================
# ì‚¬ì´ë“œë°”
# ============================================================
# íƒ€ì´í‹€ ì²˜ë¦¬ (Theology AI Labì¸ ê²½ìš° ë¶„í•  í‘œì‹œ)
title_main = APP_TITLE
title_sub = ""
if "Theology" in APP_TITLE and "AI Lab" in APP_TITLE:
    title_sub = "Theology"
    title_main = "AI Lab"

# ì„œë¸Œ íƒ€ì´í‹€ HTML ìƒì„±
if title_sub:
    sub_title_html = f'<div style="font-size: 0.8em; font-weight: 700; color: #FFFFFF; text-transform: uppercase; letter-spacing: 2px; text-shadow: 0 2px 4px rgba(0,0,0,0.3);">{title_sub}</div>'
else:
    sub_title_html = ""

# HTML ìƒì„± (ì™„ì „ ì¸ë¼ì¸ ë¬¸ìì—´ ê²°í•© ë°©ì‹)
sidebar_html = (
    '<div style="text-align: center; padding: 15px 0 25px 0;">'
    '<div style="margin-bottom: 10px;">'
    + sub_title_html +
    f'<div style="font-size: 1.5em; font-weight: 900; color: #FFFFFF; letter-spacing: 0.5px; text-shadow: 0 2px 4px rgba(0,0,0,0.3);">{title_main}</div>'
    '</div>'
    '<div style="display: inline-block; background: linear-gradient(135deg, #7C3AED 0%, #6B21A8 100%); color: white; padding: 4px 16px; border-radius: 20px; font-size: 0.7em; font-weight: 700; letter-spacing: 0.5px; box-shadow: 0 2px 8px rgba(124, 58, 237, 0.4);">'
    'v4.0.0'
    '</div>'
    '</div>'
)

st.sidebar.markdown(sidebar_html, unsafe_allow_html=True)

# í•˜ë“œì›¨ì–´ ì •ë³´ í‘œì‹œ
embedder = load_embedder()
st.sidebar.info(f" ì—”ì§„: BGE-M3 | ê°€ì†: {embedder.device.upper()}")

st.sidebar.markdown("---")



page = st.sidebar.radio(
    "ë©”ë‰´",
    ["ğŸ” ê²€ìƒ‰", "ğŸ“Š í†µê³„", "âš™ï¸ ì„¤ì •"],
    label_visibility="collapsed"
)

st.sidebar.markdown("---")

# ì˜µì‹œë””ì–¸ ì—´ê¸° ë²„íŠ¼
if st.sidebar.button("ğŸŸ£ ì˜µì‹œë””ì–¸ ì—´ê¸°", use_container_width=True):
    # ìµœì‹  ê²½ë¡œë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ ì—¬ëŸ¬ ì†ŒìŠ¤ í™•ì¸
    vault_path_final = ""
    
    # 1. ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ (ì„¤ì • í˜ì´ì§€ì—ì„œ ì…ë ¥ ì¤‘ì¸ ê°’)
    if st.session_state.get("obsidian_vault_input"):
        vault_path_final = st.session_state.get("obsidian_vault_input", "").strip()
    
    # 2. ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ì˜ ì €ì¥ëœ ì„¤ì •
    if not vault_path_final and st.session_state.get("current_settings"):
        vault_path_final = st.session_state.current_settings.get("OBSIDIAN_VAULT", "").strip()
    
    # 3. .env íŒŒì¼ ì§ì ‘ ì½ê¸° (ìµœí›„ ìˆ˜ë‹¨)
    if not vault_path_final and ENV_FILE.exists():
        with open(ENV_FILE, "r", encoding="utf-8") as f:
            for line in f:
                if line.strip().startswith("OBSIDIAN_VAULT="):
                    vault_path_final = line.strip().split("=", 1)[1].strip().strip('"').strip("'")
                    break
    
    if vault_path_final:
        import subprocess
        import platform
        from urllib.parse import quote
        
        # ì „ì²´ ê²½ë¡œë¥¼ URL ì¸ì½”ë”©í•˜ì—¬ ì‚¬ìš© (path= ë°©ì‹)
        path_encoded = quote(vault_path_final)
        obsidian_uri = f"obsidian://open?path={path_encoded}"
        
        vault_name = Path(vault_path_final).name
        st.sidebar.caption(f"ğŸš€ ì‹¤í–‰: {vault_name}")
        
        try:
            if platform.system() == "Darwin":
                subprocess.run(["open", obsidian_uri])
            elif platform.system() == "Windows":
                os.startfile(obsidian_uri)
            else:
                subprocess.run(["xdg-open", obsidian_uri])
        except Exception as e:
            st.sidebar.error(f"ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    else:
        st.sidebar.warning("âš ï¸ ì„¤ì • > Obsidian ê²½ë¡œë¥¼ ë¨¼ì € ì…ë ¥í•˜ì„¸ìš”")

st.sidebar.markdown("---")
st.sidebar.markdown(
    """
    <div style="text-align: center; font-size: 0.85em; padding-top: 10px;">
        <a href="https://www.kerygma.co.kr" target="_blank" style="color: #6B46C1; text-decoration: none; font-weight: 600; display: inline-flex; align-items: center; gap: 5px;">
            <span>â˜•</span> ì±… í•œ ê¶Œ ì‚¬ì£¼ì…”ì„œ ì‘ì›í•´ì£¼ì„¸ìš”
        </a>
        <br><br>
        <div style="color: #A0AEC0; font-size: 0.8em; line-height: 1.4;">
            Â© 2025 Kerygma Press
        </div>
    </div>
    """,
    unsafe_allow_html=True
)

# ============================================================
# AI ë¦¬í¬íŠ¸ ìƒì„± í•¨ìˆ˜
# ============================================================
def generate_ai_report(query: str, context: str, provider: str, model_name: str, api_key: str) -> str:
    """
    ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ AIê°€ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±

    Args:
        query: ê²€ìƒ‰ ì§ˆë¬¸
        context: ê²€ìƒ‰ ê²°ê³¼ í…ìŠ¤íŠ¸
        provider: API í”„ë¡œë°”ì´ë” (anthropic, openai, google)
        model_name: ëª¨ë¸ ì´ë¦„
        api_key: API í‚¤

    Returns:
        AI ìƒì„± ë¦¬í¬íŠ¸
    """
    system_prompt = """ë‹¹ì‹ ì€ ì‹ í•™ ì—°êµ¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ì‹ í•™ ë¬¸í—Œ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ í•™ìˆ ì ìœ¼ë¡œ ì •í™•í•˜ê³  ê¹Šì´ ìˆëŠ” ë¶„ì„ì„ ì œê³µí•˜ì„¸ìš”.

ê·œì¹™:
1. ì œê³µëœ ìë£Œì— ê·¼ê±°í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
2. ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš” (ì˜ˆ: "TREì— ë”°ë¥´ë©´...")
3. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”
4. í•™ìˆ ì  í†¤ì„ ìœ ì§€í•˜ì„¸ìš”
5. ìë£Œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”"""

    user_prompt = f"""ì§ˆë¬¸: {query}

ì°¸ê³  ìë£Œ:
{context}

ìœ„ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”."""

    try:
        if provider == "anthropic":
            import anthropic
            client = anthropic.Anthropic(api_key=api_key)
            response = client.messages.create(
                model=model_name,
                max_tokens=4096,
                system=system_prompt,
                messages=[{"role": "user", "content": user_prompt}]
            )
            return response.content[0].text

        elif provider == "openai":
            import openai
            client = openai.OpenAI(api_key=api_key)
            response = client.chat.completions.create(
                model=model_name,
                max_tokens=4096,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ]
            )
            return response.choices[0].message.content

        elif provider == "google":
            from google import genai
            from google.genai import types
            
            client = genai.Client(api_key=api_key)
            response = client.models.generate_content(
                model=model_name,
                contents=f"{system_prompt}\n\n{user_prompt}",
                config=types.GenerateContentConfig(max_output_tokens=4096)
            )
            return response.text

        else:
            return f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” í”„ë¡œë°”ì´ë”: {provider}"

    except Exception as e:
        return f"âŒ AI ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}"


def get_active_api_config() -> tuple:
    """í˜„ì¬ ì„¤ì •ëœ API ì •ë³´ ë°˜í™˜ (provider, model, api_key)"""
    if not ENV_FILE.exists():
        return None, None, None

    settings = {}
    with open(ENV_FILE, "r") as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith("#") and "=" in line:
                key, value = line.split("=", 1)
                settings[key.strip()] = value.strip().strip('"').strip("'")

    model_name = settings.get("RAG_MODEL", "")

    # ëª¨ë¸ëª…ìœ¼ë¡œ í”„ë¡œë°”ì´ë” ì¶”ë¡ 
    if model_name.startswith("claude"):
        provider = "anthropic"
        api_key = settings.get("ANTHROPIC_API_KEY", "")
    elif model_name.startswith("gpt"):
        provider = "openai"
        api_key = settings.get("OPENAI_API_KEY", "")
    elif model_name.startswith("gemini"):
        provider = "google"
        api_key = settings.get("GOOGLE_API_KEY", "")
    else:
        return None, None, None

    if not api_key:
        return None, None, None

    return provider, model_name, api_key


def run_pipeline(chunk_size: int, overlap: int, target_file: str = None):
    """ì¸ë±ì‹± íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë° ì‹¤ì‹œê°„ ë¡œê·¸ í‘œì‹œ"""
    try:
        import subprocess
        
        cmd = [
            sys.executable, 
            str(SCRIPT_DIR / "processor_v4.py"),
            "--inbox", str(INBOX_DIR),
            "--archive", str(ARCHIVE_DIR),
            "--db", str(DB_PATH)
        ]
        
        with st.status(f"ğŸ—ï¸ ì¸ë±ì‹± íŒŒì´í”„ë¼ì¸ ê°€ë™ ì¤‘{' (ê°œë³„ íŒŒì¼)' if target_file else ''}...", expanded=True) as log_status:
            log_output = st.empty()
            prog_container = st.empty()
            current_log = []
            
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                bufsize=1,
                universal_newlines=True
            )
            
            for line in process.stdout:
                line = line.strip()
                if not line: continue
                
                if "[PROGRESS]" in line:
                    try:
                        # [v2.7.23] ê°œì„ ëœ ì§„í–‰ë¥  íŒŒì‹± (í¼ì„¼íŠ¸ + ìƒíƒœ ë©”ì‹œì§€)
                        parts = line.split("[PROGRESS]")[1].strip()
                        pct_str = parts.split("%")[0].strip()
                        prog_val = int(pct_str)
                        # ê´„í˜¸ ì´í›„ ë˜ëŠ” % ì´í›„ì˜ ë©”ì‹œì§€ ì¶”ì¶œ
                        if "%" in parts:
                            status_msg = parts.split("%", 1)[1].strip()
                        else:
                            status_msg = ""
                        prog_container.progress(prog_val / 100, f"ì§„í–‰ë¥ : {prog_val}% {status_msg}")
                    except:
                        pass
                else:
                    current_log.append(line)
                    log_output.code("\n".join(current_log[-15:]))
                    
            process.wait()
            
            if process.returncode == 0:
                log_status.update(label="âœ… ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!", state="complete", expanded=False)
                st.success("ğŸ‰ ì„œì¬ ì—…ë°ì´íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                load_db.clear()
                load_lemma_index.clear()
                st.session_state.page_mappings = {}
                
                # [v2.7.23] ì™„ë£Œ í›„ ì•ˆë‚´ë¥¼ ìœ„í•œ ì„¸ì…˜ ìƒíƒœ ì„¤ì • ë° í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨
                st.session_state["pipeline_completed"] = True
                st.rerun()  # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨ìœ¼ë¡œ ìƒë‹¨ ì•ˆë‚´ íŒ¨ë„ í‘œì‹œ
            else:
                log_status.update(label="âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ", state="error")
                st.error(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨ (Exit Code: {process.returncode})")
                
    except Exception as e:
        st.error(f"âŒ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")

# ============================================================
# ê²€ìƒ‰ í˜ì´ì§€
# ============================================================
if page == "ğŸ” ê²€ìƒ‰":
    st.markdown(f"""
        <div style="padding-bottom: 25px;">
            <h1 style="color: #2D3748; font-weight: 800; letter-spacing: -0.5px; margin-bottom: 0;">ğŸ” {APP_TITLE} ê²€ìƒ‰</h1>
            <p style="color: #718096; font-size: 1.1em; font-weight: 400;">ê¸°ë¡ëœ ì§€í˜œì˜ ë°”ë‹¤ì—ì„œ í•„ìš”í•œ ë¬¸ì¥ì„ ê±´ì ¸ì˜¬ë¦¬ì„¸ìš”.</p>
        </div>
    """, unsafe_allow_html=True)


    client, collection = load_db(str(DB_PATH))

    if collection is None:
        st.warning("âš ï¸ ë°ì´í„°ë² ì´ìŠ¤ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ë¨¼ì € íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
    else:
        # [Cloud Edition] ì²­í¬ ìˆ˜ëŠ” í†µê³„ í˜ì´ì§€ì—ì„œ í™•ì¸
        pass

        # ê²€ìƒ‰ ì…ë ¥
        query = st.text_input(
            "ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”",
            placeholder="ì˜ˆ: ì¹­ì˜, Gnade, Rechtfertigung..."
        )

        # ê²€ìƒ‰ ì˜µì…˜
        col_opt1, col_opt2, col_opt3 = st.columns([2, 2, 1])
        with col_opt1:
            use_trilingual = st.checkbox("ğŸŒ 3ì¤‘ ì–¸ì–´ í™•ì¥ (í•œ/ì˜/ë…)", value=True, help="ì‹ í•™ ìš©ì–´ë¥¼ í•œêµ­ì–´, ì˜ì–´, ë…ì¼ì–´ë¡œ ìë™ í™•ì¥í•©ë‹ˆë‹¤")
        with col_opt2:
            use_dual_search = st.checkbox("ğŸ” ì´ì¤‘ ê²€ìƒ‰ (Vector+JSON)", value=True, help="ë²¡í„° ê²€ìƒ‰ê³¼ í‚¤ì›Œë“œ ê²€ìƒ‰ì„ ë™ì‹œ ìˆ˜í–‰í•©ë‹ˆë‹¤")
        with col_opt3:
            n_results = st.selectbox("ê²°ê³¼ ìˆ˜", [5, 10, 20], index=0)

        # 3ì¤‘ ì–¸ì–´ í™•ì¥ í‘œì‹œ
        if query and use_trilingual:
            expander = load_query_expander()
            if expander:
                expanded = expander.expand(query)
                if expanded.matched_concepts:
                    with st.expander(f"ğŸŒ ì¿¼ë¦¬ í™•ì¥: {', '.join(expanded.matched_concepts)}", expanded=False):
                        col_lang1, col_lang2, col_lang3 = st.columns(3)
                        with col_lang1:
                            st.caption("ğŸ‡°ğŸ‡· í•œêµ­ì–´")
                            st.write(", ".join(expanded.korean[:5]))
                        with col_lang2:
                            st.caption("ğŸ‡ºğŸ‡¸ English")
                            st.write(", ".join(expanded.english[:5]))
                        with col_lang3:
                            st.caption("ğŸ‡©ğŸ‡ª Deutsch")
                            st.write(", ".join(expanded.german[:5]))

        if query:
            with st.spinner("ê²€ìƒ‰ ì¤‘..."):
                # 3ì¤‘ ì–¸ì–´ í™•ì¥ ì ìš©
                search_queries = [query]
                if use_trilingual:
                    expander = load_query_expander()
                    if expander:
                        search_queries = expander.get_embedding_queries(query, max_q=3)
                
                all_results = []
                
                # ì´ì¤‘ ê²€ìƒ‰ ëª¨ë“œ
                if use_dual_search:
                    try:
                        from utils.dual_search import DualSearchEngine
                        dual_engine = DualSearchEngine(
                            db_path=str(DB_PATH),
                            archive_path=str(ARCHIVE_DIR),
                            use_trilingual=use_trilingual
                        )
                        dual_results = dual_engine.search(query, n_results=n_results * 2)
                        
                        # DualSearchEngine ê²°ê³¼ë¥¼ Document í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                        from langchain_core.documents import Document
                        for r in dual_results:
                            doc = Document(
                                page_content=r.content,
                                metadata={
                                    "source": r.source,
                                    "author": r.author,
                                    "doc_type": r.doc_type,
                                    "page": r.page,
                                    "search_method": r.method,
                                    **r.metadata
                                }
                            )
                            all_results.append(doc)
                    except Exception as e:
                        logger.warning(f"Dual search failed, falling back: {e}")
                        use_dual_search = False  # Fallback to normal search
                
                # ì¼ë°˜ ë²¡í„° ê²€ìƒ‰ (fallback ë˜ëŠ” ì´ì¤‘ ê²€ìƒ‰ ë¹„í™œì„±í™” ì‹œ)
                if not use_dual_search or not all_results:
                    searcher = load_searcher(str(DB_PATH))
                    for sq in search_queries:
                        results_docs = searcher.search(sq)
                        all_results.extend(results_docs)
                
                # ì¤‘ë³µ ì œê±° (content ê¸°ì¤€)
                seen_contents = set()
                unique_results = []
                for doc in all_results:
                    content_key = doc.page_content[:100]
                    if content_key not in seen_contents:
                        seen_contents.add(content_key)
                        unique_results.append(doc)
                
                # ê¸°ì¡´ ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜ (ê¸°ì¡´ UI í˜¸í™˜ì„± ìœ ì§€)
                documents = [d.page_content for d in unique_results[:n_results]]
                metadatas = [d.metadata for d in unique_results[:n_results]]
                results = {'documents': [documents], 'metadatas': [metadatas]}

            if results['documents'] and results['documents'][0]:
                st.markdown(f"### ê²€ìƒ‰ ê²°ê³¼ ({len(results['documents'][0])}ê±´)")

                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                # ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„± í•¨ìˆ˜
                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                def generate_search_report(query: str, results: dict) -> str:
                    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ë¡œ ë³€í™˜"""
                    report_lines = [
                        f"# ê²€ìƒ‰ ë¦¬í¬íŠ¸: {query}",
                        f"",
                        f"**ê²€ìƒ‰ì¼**: {datetime.now().strftime('%Y-%m-%d %H:%M')}",
                        f"**ê²°ê³¼ ìˆ˜**: {len(results['documents'][0])}ê±´",
                        f"",
                        "---",
                        "",
                    ]

                    for i, doc in enumerate(results['documents'][0]):
                        meta = results['metadatas'][0][i]
                        source = meta.get('source', 'Unknown')
                        # ë©”íƒ€ë°ì´í„° í‚¤ í˜¸í™˜ì„± ì²˜ë¦¬ (page or page_number)
                        raw_page = meta.get('page', meta.get('page_number', '?'))
                        try:
                            page_num = int(raw_page) + PAGE_OFFSET if str(raw_page).isdigit() else raw_page
                        except:
                            page_num = raw_page
                        
                        lemma = meta.get('lemma', '')
                        category = meta.get('category', '')

                        report_lines.append(f"## [{i+1}] {source} - p.{page_num}")
                        if lemma:
                            report_lines.append(f"**í‘œì œì–´**: {lemma}")
                        if category:
                            report_lines.append(f"**ë¶„ë¥˜**: {category}")
                        report_lines.append("")
                        report_lines.append(doc)
                        report_lines.append("")
                        report_lines.append("---")
                        report_lines.append("")

                    report_lines.append(f"*Generated by {APP_TITLE}*")
                    return "\n".join(report_lines)

                # ë¦¬í¬íŠ¸ ìƒì„±
                markdown_report = generate_search_report(query, results)

                # ì˜µì‹œë””ì–¸ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸° í•¨ìˆ˜
                def save_to_obsidian(content: str, filename: str) -> tuple:
                    """ì˜µì‹œë””ì–¸ Vaultì— ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì €ì¥"""
                    # ê²½ë¡œ í™•ì¸ (ì‚¬ì´ë“œë°” ë²„íŠ¼ê³¼ ë™ì¼í•œ ë°©ì‹)
                    vault_path_str = ""
                    
                    # 1. ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ (ì„¤ì • í˜ì´ì§€ì—ì„œ ì…ë ¥ ì¤‘ì¸ ê°’)
                    if st.session_state.get("obsidian_vault_input"):
                        vault_path_str = st.session_state.get("obsidian_vault_input", "").strip()
                    
                    # 2. ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ì˜ ì €ì¥ëœ ì„¤ì •
                    if not vault_path_str and st.session_state.get("current_settings"):
                        vault_path_str = st.session_state.current_settings.get("OBSIDIAN_VAULT", "").strip()
                    
                    # 3. .env íŒŒì¼ ì§ì ‘ ì½ê¸°
                    if not vault_path_str and ENV_FILE.exists():
                        with open(ENV_FILE, "r", encoding="utf-8") as f:
                            for line in f:
                                if line.strip().startswith("OBSIDIAN_VAULT="):
                                    vault_path_str = line.strip().split("=", 1)[1].strip().strip('"').strip("'")
                                    break
                    
                    if not vault_path_str:
                        return False, "Obsidian Vault ê²½ë¡œê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

                    vault_path = Path(vault_path_str)
                    if not vault_path.exists():
                        return False, f"Vault ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {vault_path}"

                    # íŒŒì¼ëª… ì •ë¦¬ (íŠ¹ìˆ˜ë¬¸ì ì œê±°)
                    safe_filename = "".join(c if c.isalnum() or c in "._- " else "_" for c in filename)
                    file_path = vault_path / f"{safe_filename}.md"

                    try:
                        with open(file_path, "w", encoding="utf-8") as f:
                            f.write(content)
                        return True, str(file_path)
                    except Exception as e:
                        return False, str(e)

                # ë‹¤ìš´ë¡œë“œ/ì˜µì‹œë””ì–¸/AI ë¦¬í¬íŠ¸ ë²„íŠ¼
                export_col1, export_col2, export_col3 = st.columns([1, 1, 1])
                with export_col1:
                    st.download_button(
                        label="ğŸ“¥ ë‹¤ìš´ë¡œë“œ",
                        data=markdown_report,
                        file_name=f"search_{query.replace(' ', '_')}_{datetime.now().strftime('%Y%m%d')}.md",
                        mime="text/markdown",
                        key="download_report"
                    )
                with export_col2:
                    if st.button("ğŸŸ£ ì˜µì‹œë””ì–¸", key="obsidian_report"):
                        filename = f"ê²€ìƒ‰_{query}_{datetime.now().strftime('%Y%m%d_%H%M')}"
                        success, result = save_to_obsidian(markdown_report, filename)
                        if success:
                            st.success(f"âœ… ì €ì¥ë¨: {Path(result).name}")
                        else:
                            st.error(f"âŒ {result}")
                with export_col3:
                    # API ì„¤ì • í™•ì¸
                    provider, model_name, api_key = get_active_api_config()
                    if provider and api_key:
                        if st.button("ğŸ¤– AI ë¶„ì„", key="ai_report"):
                            st.session_state["generate_ai_report"] = True
                    else:
                        if st.button("ğŸ¤– AI ë¶„ì„", key="ai_report_disabled", disabled=True):
                            pass
                        st.caption("âš ï¸ ì„¤ì •ì—ì„œ API í‚¤ ë“±ë¡ í•„ìš”")

                # AI ë¦¬í¬íŠ¸ ìƒì„±
                if st.session_state.get("generate_ai_report"):
                    provider, model_name, api_key = get_active_api_config()
                    if provider and api_key:
                        with st.spinner(f"ğŸ¤– AI ë¶„ì„ ì¤‘... ({model_name})"):
                            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                            context_parts = []
                            for i, doc in enumerate(results['documents'][0]):
                                meta = results['metadatas'][0][i]
                                source = meta.get('source', 'Unknown')
                                raw_page = meta.get('page', meta.get('page_number', '?'))
                                try:
                                    page_num = int(raw_page) + PAGE_OFFSET if str(raw_page).isdigit() else raw_page
                                except:
                                    page_num = raw_page
                                context_parts.append(f"[ì¶œì²˜: {source}, p.{page_num}]\n{doc}")

                            context = "\n\n---\n\n".join(context_parts)

                            # AI ë¦¬í¬íŠ¸ ìƒì„±
                            ai_report = generate_ai_report(query, context, provider, model_name, api_key)

                        st.session_state["ai_report_content"] = ai_report
                        st.session_state["generate_ai_report"] = False
                        st.rerun()

                # AI ë¦¬í¬íŠ¸ í‘œì‹œ
                if st.session_state.get("ai_report_content"):
                    st.markdown("---")
                    st.markdown("### ğŸ¤– AI ë¶„ì„ ë¦¬í¬íŠ¸")
                    st.markdown(st.session_state["ai_report_content"])

                    # AI ë¦¬í¬íŠ¸ ë‚´ë³´ë‚´ê¸° ë²„íŠ¼
                    ai_col1, ai_col2, ai_col3 = st.columns([1, 1, 1])
                    with ai_col1:
                        st.download_button(
                            label="ğŸ“¥ AI ë¦¬í¬íŠ¸ ë‹¤ìš´ë¡œë“œ",
                            data=st.session_state["ai_report_content"],
                            file_name=f"ai_report_{query.replace(' ', '_')}_{datetime.now().strftime('%Y%m%d')}.md",
                            mime="text/markdown",
                            key="download_ai_report"
                        )
                    with ai_col2:
                        if st.button("ğŸŸ£ ì˜µì‹œë””ì–¸ ì €ì¥", key="obsidian_ai_report"):
                            filename = f"AIë¶„ì„_{query}_{datetime.now().strftime('%Y%m%d_%H%M')}"
                            # í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ìë™ìœ¼ë¡œ ê²½ë¡œë¥¼ ê°€ì ¸ì˜´
                            success, result = save_to_obsidian(st.session_state["ai_report_content"], filename)
                            if success:
                                st.success(f"âœ… ì €ì¥ë¨: {Path(result).name}")
                            else:
                                st.error(f"âŒ {result}")
                    with ai_col3:
                        if st.button("ğŸ—‘ï¸ ë‹«ê¸°", key="close_ai_report"):
                            st.session_state["ai_report_content"] = None
                            st.rerun()

                    st.markdown("---")

                st.markdown("---")

                # ê°œë³„ ê²°ê³¼ í‘œì‹œ
                for i, doc in enumerate(results['documents'][0]):
                    meta = results['metadatas'][0][i]

                    # ì¶œì²˜ ì •ë³´
                    source = meta.get('source', 'Unknown')
                    raw_page = meta.get('page', meta.get('page_number', '?'))
                    page_num = raw_page
                    
                    lemma = meta.get('lemma', '')
                    category = meta.get('category', '')
                    author = meta.get('author', '')

                    # í•˜ì´ë¼ì´íŒ… ì²˜ë¦¬ (ë‹¨ìˆœ í…ìŠ¤íŠ¸ êµì²´)
                    highlighted_doc = doc
                    if query:
                        # í•œêµ­ì–´/ì˜ì–´/ë…ì¼ì–´ í‚¤ì›Œë“œ ê°•ì¡°
                        keywords = query.split()
                        for kw in keywords:
                            if len(kw) > 1:
                                pattern = re.compile(re.escape(kw), re.IGNORECASE)
                                highlighted_doc = pattern.sub(f"<mark style='background-color: #FEF08A; border-radius: 2px; padding: 0 2px;'>{kw}</mark>", highlighted_doc)

                    # ì¹´ë“œ í˜•ì‹ìœ¼ë¡œ í‘œì‹œ
                    with st.expander(f"**[{i+1}] {source}** - p.{page_num} {f'| {lemma}' if lemma else ''}", expanded=(i==0)):
                        if category or author:
                            st.caption(f"ğŸ“‚ {category} {'| âœï¸ ' + author if author else ''}")
                        st.markdown(highlighted_doc, unsafe_allow_html=True)
                        
                        # ì¶”ê°€ ë©”íƒ€ë°ì´í„° ì •ë³´ (ê³ ê¸‰ ì‚¬ìš©ììš©)
                        if st.checkbox(f"ë©”íƒ€ë°ì´í„° ë³´ê¸° ##{i}", key=f"meta_{i}"):
                            st.json(meta)
            else:
                st.info("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ============================================================
# íŒŒì¼ ì—…ë¡œë“œ í˜ì´ì§€
# ============================================================
# ============================================================
# íŒŒì¼ ì—…ë¡œë“œ í˜ì´ì§€ (Cloud Edition - Disabled)
# ============================================================
# elif page == "ğŸ“¤ íŒŒì¼ ì—…ë¡œë“œ":
#     st.error("â›”ï¸ í´ë¼ìš°ë“œ ë²„ì „ì—ì„œëŠ” íŒŒì¼ ì—…ë¡œë“œë¥¼ Colabì—ì„œ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
#     st.info("Desktop/MS_Dev.nosync/Theology_AI_Lab_v4/01_Library/inbox í´ë”ì— íŒŒì¼ì„ ë„£ê³  Colab ì¸ë±ì„œë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")

# ============================================================
# í†µê³„ í˜ì´ì§€
# ============================================================
elif page == "ğŸ“Š í†µê³„":
    st.markdown("""
        <div style="padding-bottom: 25px;">
            <h1 style="color: #2D3748; font-weight: 800; letter-spacing: -0.5px; margin-bottom: 0;">ğŸ“Š ì„œì¬ í˜„í™©</h1>
            <p style="color: #718096; font-size: 1.1em; font-weight: 400;">í˜„ì¬ ì¸ë±ì‹±ëœ ì‹ í•™ ìë£Œë“¤ì˜ í†µê³„ì™€ ë¶„í¬ì…ë‹ˆë‹¤.</p>
        </div>
    """, unsafe_allow_html=True)
    
    client, collection = load_db(str(DB_PATH))
    index_data = load_lemma_index(str(LEMMA_INDEX_PATH))

    col1, col2, col3 = st.columns(3)

    with col1:
        st.metric(
            "ğŸ“„ ì¸ë±ì‹±ëœ ì²­í¬",
            f"{collection.count():,}ê°œ" if collection else "0ê°œ"
        )

    with col2:
        lemma_count = len(index_data.get("entries", {})) if index_data else 0
        st.metric("ğŸ“– í‘œì œì–´", f"{lemma_count:,}ê°œ")

    with col3:
        # ë³´ê´€ëœ ì²­í‚¹ JSON ë¬¸ì„œ ìˆ˜ (ì¸ë±ì‹± ì™„ë£Œëœ ë„ì„œ)
        archive_files = []
        if ARCHIVE_DIR.exists():
            archive_files = [f for f in ARCHIVE_DIR.glob("*.json") if not f.name.startswith("lemma_")]
        st.metric("ğŸ“š ë³´ê´€ ë¬¸ì„œ", f"{len(archive_files)}ê¶Œ")

    st.markdown("---")

    # ì†ŒìŠ¤ë³„ ë¶„í¬ (ChromaDBì—ì„œ ì§ì ‘ ì¡°íšŒ)
    by_source = get_sources_from_db()

    if by_source:
        import pandas as pd

        total_chunks = sum(info.get("count", 0) for info in by_source.values())

        st.markdown(f"### ğŸ“š ì¸ë±ì‹± ì†ŒìŠ¤ ({len(by_source)}ê°œ, ì´ {total_chunks:,} ì²­í¬)")

        # ì†ŒìŠ¤ ìœ í˜• ë¶„ë¥˜ (ìë™ ì¶”ë¡ )
        SOURCE_TYPES = {
            "TRE": "ì‹ í•™ë°±ê³¼", "RGG": "ì‹ í•™ë°±ê³¼", "EKL": "ì‹ í•™ë°±ê³¼",
            "TDNT": "ì„±ì„œì‚¬ì „", "NIDNTT": "ì„±ì„œì‚¬ì „", "EDNT": "ì„±ì„œì‚¬ì „",
            "ThWAT": "ì„±ì„œì‚¬ì „", "EWNT": "ì„±ì„œì‚¬ì „",
            "HWPh": "ì² í•™ì‚¬ì „",
            "KD": "êµì˜í•™",
        }

        # ë°ì´í„° ì¤€ë¹„
        source_data = []
        for source, info in by_source.items():
            volumes = info.get("volumes", [])
            source_data.append({
                "ì†ŒìŠ¤": source,
                "ìœ í˜•": SOURCE_TYPES.get(source, "ê¸°íƒ€"),
                "ê¶Œìˆ˜": len(volumes) if volumes else 1,
                "ì²­í¬": info.get("count", 0),
            })

        df = pd.DataFrame(source_data).sort_values("ì²­í¬", ascending=False)

        # í•„í„° UI
        col1, col2 = st.columns([2, 1])
        with col1:
            search_source = st.text_input("ì†ŒìŠ¤ ê²€ìƒ‰", placeholder="ì˜ˆ: TRE, RGG...", key="source_search")
        with col2:
            all_types = ["ì „ì²´"] + sorted(df["ìœ í˜•"].unique().tolist())
            selected_type = st.selectbox("ìœ í˜• í•„í„°", all_types, key="type_filter")

        # í•„í„° ì ìš©
        filtered_df = df.copy()
        if search_source:
            filtered_df = filtered_df[filtered_df["ì†ŒìŠ¤"].str.contains(search_source, case=False)]
        if selected_type != "ì „ì²´":
            filtered_df = filtered_df[filtered_df["ìœ í˜•"] == selected_type]

        # í…Œì´ë¸” í‘œì‹œ (ìƒìœ„ 10ê°œ + ë”ë³´ê¸°)
        show_all = st.checkbox(f"ì „ì²´ í‘œì‹œ ({len(filtered_df)}ê°œ)", key="show_all_sources")
        display_df = filtered_df if show_all else filtered_df.head(10)

        st.dataframe(
            display_df,
            column_config={
                "ì†ŒìŠ¤": st.column_config.TextColumn("ì†ŒìŠ¤", width="medium"),
                "ìœ í˜•": st.column_config.TextColumn("ìœ í˜•", width="small"),
                "ê¶Œìˆ˜": st.column_config.NumberColumn("ê¶Œìˆ˜", format="%d"),
                "ì²­í¬": st.column_config.NumberColumn("ì²­í¬", format="%d"),
            },
            hide_index=True,
            width="stretch"
        )

        # ì°¨íŠ¸ (ìƒìœ„ 10ê°œë§Œ)
        if len(df) > 0:
            with st.expander("ğŸ“Š ì²­í¬ ë¶„í¬ ì°¨íŠ¸", expanded=False):
                chart_df = df.head(10).set_index("ì†ŒìŠ¤")[["ì²­í¬"]]
                st.bar_chart(chart_df)

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ì†ŒìŠ¤ ê´€ë¦¬ ì„¹ì…˜
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        st.markdown("---")
        st.markdown("### ğŸ”§ ì†ŒìŠ¤ ê´€ë¦¬")

        # ì†ŒìŠ¤ ì„ íƒ
        source_list = sorted(by_source.keys())
        selected_source = st.selectbox(
            "ê´€ë¦¬í•  ì†ŒìŠ¤ ì„ íƒ",
            ["ì„ íƒí•˜ì„¸ìš”..."] + source_list,
            key="manage_source_select"
        )

        if selected_source and selected_source != "ì„ íƒí•˜ì„¸ìš”...":
            source_info = by_source.get(selected_source, {})
            chunk_count = source_info.get("count", 0)

            col_info, col_actions = st.columns([2, 1])

            with col_info:
                st.info(f"**{selected_source}**: {chunk_count:,}ê°œ ì²­í¬")
                # ì•„ì¹´ì´ë¸Œ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                archive_file = ARCHIVE_DIR / f"{selected_source}.json"
                if archive_file.exists():
                    st.success(f"ğŸ“¦ ì•„ì¹´ì´ë¸Œ íŒŒì¼ ì¡´ì¬: {archive_file.name}")
                else:
                    st.warning("âš ï¸ ì•„ì¹´ì´ë¸Œ íŒŒì¼ ì—†ìŒ (ì¬ì¸ë±ì‹± ë¶ˆê°€)")

            with col_actions:
                st.markdown("**ì‘ì—… ì„ íƒ:**")

                # ì‚­ì œ ë²„íŠ¼
                if st.button("ğŸ—‘ï¸ DBì—ì„œ ì‚­ì œ", key=f"delete_{selected_source}", type="secondary"):
                    st.session_state["confirm_delete"] = selected_source

                # ì¬ì¸ë±ì‹± ë²„íŠ¼ (ì•„ì¹´ì´ë¸Œ íŒŒì¼ ìˆì„ ë•Œë§Œ)
                if archive_file.exists():
                    if st.button("ğŸ”„ ì¬ì¸ë±ì‹±", key=f"reindex_{selected_source}", type="primary"):
                        st.session_state["confirm_reindex"] = selected_source

            # ì‚­ì œ í™•ì¸ ë‹¤ì´ì–¼ë¡œê·¸
            if st.session_state.get("confirm_delete") == selected_source:
                st.warning(f"âš ï¸ **'{selected_source}'**ì˜ {chunk_count:,}ê°œ ì²­í¬ë¥¼ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
                col_yes, col_no = st.columns(2)
                with col_yes:
                    if st.button("âœ… ì‚­ì œ í™•ì¸", key="confirm_delete_yes", type="primary"):
                        with st.spinner("ì‚­ì œ ì¤‘..."):
                            try:
                                deleted = delete_source_from_db(selected_source)
                                st.success(f"âœ… '{selected_source}' ì‚­ì œ ì™„ë£Œ ({deleted}ê°œ ì²­í¬)")
                                st.session_state["confirm_delete"] = None
                                st.rerun()
                            except Exception as e:
                                st.error(f"ì‚­ì œ ì‹¤íŒ¨: {e}")
                with col_no:
                    if st.button("âŒ ì·¨ì†Œ", key="confirm_delete_no"):
                        st.session_state["confirm_delete"] = None
                        st.rerun()

            # ì¬ì¸ë±ì‹± í™•ì¸ ë‹¤ì´ì–¼ë¡œê·¸
            if st.session_state.get("confirm_reindex") == selected_source:
                st.info(f"ğŸ”„ **'{selected_source}'** ì¬ì¸ë±ì‹±: ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ì•„ì¹´ì´ë¸Œì—ì„œ ë‹¤ì‹œ ì¸ë±ì‹±í•©ë‹ˆë‹¤.")
                col_yes, col_no = st.columns(2)
                with col_yes:
                    if st.button("âœ… ì¬ì¸ë±ì‹± ì‹œì‘", key="confirm_reindex_yes", type="primary"):
                        with st.spinner("ì¬ì¸ë±ì‹± ì¤‘..."):
                            try:
                                result = reindex_source(selected_source)
                                st.success(f"âœ… ì¬ì¸ë±ì‹± ì™„ë£Œ: {result}")
                                st.session_state["confirm_reindex"] = None
                                st.rerun()
                            except Exception as e:
                                st.error(f"ì¬ì¸ë±ì‹± ì‹¤íŒ¨: {e}")
                with col_no:
                    if st.button("âŒ ì·¨ì†Œ", key="confirm_reindex_no"):
                        st.session_state["confirm_reindex"] = None
                        st.rerun()

    else:
        st.info("ì†ŒìŠ¤ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # ê²½ë¡œ ì •ë³´
    st.markdown("---")
    st.markdown("### ğŸ“‚ ë°ì´í„° ì €ì¥ì†Œ ê²½ë¡œ (Google Drive Cloud)")
    st.code(f"""
[Local Inbox] : {INBOX_DIR}
[Cloud Archive]: {ARCHIVE_DIR}
[Cloud DB]     : {DB_PATH}
""", language="text")

# ============================================================
# ì„¤ì • í˜ì´ì§€
# ============================================================
elif page == "âš™ï¸ ì„¤ì •":
    st.markdown("""
        <div style="padding-bottom: 25px;">
            <h1 style="color: #2D3748; font-weight: 800; letter-spacing: -0.5px; margin-bottom: 0;">âš™ï¸ ì„œì¬ í™˜ê²½ ì„¤ì •</h1>
            <p style="color: #718096; font-size: 1.1em; font-weight: 400;">AI ëª¨ë¸, API í‚¤, ê·¸ë¦¬ê³  ì˜µì‹œë””ì–¸ ë³¼íŠ¸ ê²½ë¡œë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.</p>
        </div>
    """, unsafe_allow_html=True)

    # .env íŒŒì¼ ê²½ë¡œ
    ENV_FILE = KIT_ROOT / ".env"

    # í˜„ì¬ ì„¤ì • ë¡œë“œ (ì „ì—­ ì„¤ì • ê¸°ë°˜)
    def load_env_settings():
        return load_global_settings()

    def save_env_settings(settings_to_save: dict):
        """ì„¤ì •ì„ .env íŒŒì¼ì— ì €ì¥"""
        # ê¸°ì¡´ ë‚´ìš© ì½ê¸°
        existing_lines = []
        existing_keys = set()
        if ENV_FILE.exists():
            with open(ENV_FILE, "r", encoding="utf-8") as f:
                for line in f:
                    stripped = line.strip()
                    if stripped and not stripped.startswith("#") and "=" in stripped:
                        key = stripped.split("=", 1)[0].strip()
                        if key in settings_to_save:
                            existing_keys.add(key)
                            existing_lines.append(f"{key}={settings_to_save[key]}\n")
                        else:
                            existing_lines.append(line)
                    else:
                        existing_lines.append(line)

        # ìƒˆ í‚¤ ì¶”ê°€
        for key, value in settings_to_save.items():
            if key not in existing_keys:
                existing_lines.append(f"{key}={value}\n")

        # ì €ì¥
        with open(ENV_FILE, "w", encoding="utf-8") as f:
            f.writelines(existing_lines)

    # ì„¸ì…˜ ìƒíƒœ í™œìš©í•˜ì—¬ ì…ë ¥ê°’ ìœ ì§€
    if "settings_loaded" not in st.session_state:
        st.session_state.current_settings = load_env_settings()
        st.session_state.settings_loaded = True

    settings = st.session_state.current_settings

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ì•± íƒ€ì´í‹€ ì„¤ì •
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("### ğŸ“ ì•± íƒ€ì´í‹€ ì„¤ì •")
    st.caption("ê²€ìƒ‰ í˜ì´ì§€ì— í‘œì‹œë˜ëŠ” íƒ€ì´í‹€ì„ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    app_title_input = st.text_input(
        "ì•± íƒ€ì´í‹€",
        value=settings.get("APP_TITLE", "Theology AI Lab"),
        placeholder="ì˜ˆ: My Digi-Th-Library",
        key="app_title_input"
    )

    st.markdown("---")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ì˜µì‹œë””ì–¸ ì„¤ì • (v2.3: ë‹¤ì¤‘ ë³¼íŠ¸ ì´ë ¥ ê´€ë¦¬)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("### ğŸ“ Obsidian ì—°ë™ ì„¤ì •")
    st.caption("ê²€ìƒ‰ ê²°ê³¼ë¥¼ Obsidian ë…¸íŠ¸ë¡œ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    VAULT_HISTORY_FILE = SCRIPT_DIR / "vault_history.json"
    
    def load_vault_history():
        if VAULT_HISTORY_FILE.exists():
            try:
                with open(VAULT_HISTORY_FILE, "r", encoding="utf-8") as f:
                    return json.load(f)
            except:
                return []
        return []

    def save_vault_history(vault_path):
        history = load_vault_history()
        if vault_path in history:
            history.remove(vault_path)
        history.insert(0, vault_path)
        history = history[:5]  # ìµœê·¼ 5ê°œë§Œ ìœ ì§€
        with open(VAULT_HISTORY_FILE, "w", encoding="utf-8") as f:
            json.dump(history, f, ensure_ascii=False, indent=2)

    vault_history = load_vault_history()
    
    # 1. ìµœê·¼ ì‚¬ìš©ëœ ë³¼íŠ¸ ì„ íƒ
    selected_vault_from_history = ""
    if vault_history:
        selected_vault_from_history = st.selectbox(
            "ìµœê·¼ ì‚¬ìš©ëœ Vault",
            options=["-- ì§ì ‘ ì…ë ¥ --"] + vault_history,
            index=0
        )

    # 2. ë³¼íŠ¸ ê²½ë¡œ ì…ë ¥
    if selected_vault_from_history != "-- ì§ì ‘ ì…ë ¥ --":
        default_vault = selected_vault_from_history
    else:
        default_vault = settings.get("OBSIDIAN_VAULT", "")
    
    obsidian_vault_input = st.text_input(
        "Obsidian Vault ê²½ë¡œ",
        value=default_vault,
        placeholder="ì˜ˆ: /Users/username/Documents/MyVault",
        key="obsidian_vault_input",
        help="Obsidian Vaultì˜ ì „ì²´ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”"
    )

    # ê²½ë¡œê°€ ë³€ê²½/ì…ë ¥ë˜ë©´ ì´ë ¥ì— ì¶”ê°€
    if obsidian_vault_input and obsidian_vault_input != settings.get("OBSIDIAN_VAULT", ""):
        vault_path = Path(obsidian_vault_input)
        if vault_path.exists():
            save_vault_history(obsidian_vault_input)

    # ê²½ë¡œ ì¡´ì¬ í™•ì¸
    if obsidian_vault_input:
        vault_path = Path(obsidian_vault_input)
        if vault_path.exists():
            st.success(f"âœ… Vault í™•ì¸ë¨: {vault_path.name}")
        else:
            st.warning("âš ï¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")

    st.markdown("---")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ë°ì´í„° ê´€ë¦¬ (DB ì´ˆê¸°í™”) [v4.3 ì¶”ê°€]
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("### ğŸ—‘ï¸ ë°ì´í„° ê´€ë¦¬")
    st.caption(f"í´ë¼ìš°ë“œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤({DB_PATH})ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
    st.warning("âš ï¸ ì£¼ì˜: Google Driveì™€ ë™ê¸°í™”ëœ DBê°€ ì¦‰ì‹œ ì‚­ì œë©ë‹ˆë‹¤. ì‹ ì¤‘í•˜ê²Œ ì§„í–‰í•˜ì„¸ìš”!")
    
    col_reset1, col_reset2 = st.columns([1, 2])
    with col_reset1:
        if st.button("ğŸ§¨ DB ì´ˆê¸°í™” (Reset)", type="primary", use_container_width=True):
            try:
                import shutil
                
                # 1. ChromaDB ì‚­ì œ (ì•„ì¹´ì´ë¸ŒëŠ” ë³´ì¡´)
                if DB_PATH.exists():
                    shutil.rmtree(DB_PATH)
                    st.toast("ğŸ§¹ ë²¡í„° DBê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.", icon="ğŸ—‘ï¸")
                
                # 3. í´ë” ì¬ìƒì„±
                DB_PATH.mkdir(parents=True, exist_ok=True)
                
                # 4. ìºì‹œ ë¬´íš¨í™” (í†µê³„ ì¦‰ì‹œ ë°˜ì˜)
                st.cache_resource.clear()
                st.cache_data.clear()
                
                st.success("âœ… ì´ˆê¸°í™” ì™„ë£Œ! ì´ì œ ë‹¤ì‹œ ì¸ë±ì‹±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ì•„ì¹´ì´ë¸Œ íŒŒì¼ì€ ë³´ì¡´ë¨)")
                st.rerun() # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨ìœ¼ë¡œ í†µê³„ 0 ë°˜ì˜
                
            except Exception as e:
                st.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    st.markdown("---")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ğŸš€ 2. êµ¬ê¸€ ë“œë¼ì´ë¸Œ í´ë¼ìš°ë“œ ì—°ê²°
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("### â˜ï¸ í´ë¼ìš°ë“œ ì—°ê²° (í•„ìˆ˜)")
    st.caption("êµ¬ê¸€ ë“œë¼ì´ë¸Œê°€ ì„¤ì¹˜ëœ ê²½ë¡œë¥¼ ì—°ê²°í•´ì•¼ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")

    # í˜„ì¬ ì„¤ì •ëœ ê²½ë¡œê°€ ìœ íš¨í•œì§€ í™•ì¸
    is_path_valid = INBOX_DIR.exists() and DB_PATH.exists()
    
    if is_path_valid:
        st.success(f"âœ… ì—°ê²°ë¨: `{INBOX_DIR.parent.parent}`")
    else:
        st.error("âŒ ì—°ê²° ì•ˆ ë¨: ê²½ë¡œ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    
    with st.expander("ğŸ“‚ êµ¬ê¸€ ë“œë¼ì´ë¸Œ ê²½ë¡œ ì„¤ì • ë§ˆë²•ì‚¬", expanded=not is_path_valid):
        st.info("ğŸ’¡ êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë‚´ `Theology_AI_LAB` í´ë”ì˜ ì „ì²´ ê²½ë¡œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        # ê¸°ë³¸ê°’: í˜„ì¬ ê²½ë¡œê°€ ì ˆëŒ€ê²½ë¡œë¼ë©´ í‘œì‹œ
        current_root_str = str(INBOX_DIR.parent.parent) if str(INBOX_DIR).startswith("/") else ""
        
        cloud_root_input = st.text_input(
            "Theology_AI_LAB í´ë” ê²½ë¡œ (ì „ì²´ ê²½ë¡œ)",
            value=current_root_str,
            placeholder="/Users/ì‚¬ìš©ì/Library/CloudStorage/GoogleDrive-ë©”ì¼/ë‚´ ë“œë¼ì´ë¸Œ/Theology_AI_LAB",
            help="Finderì—ì„œ í´ë”ë¥¼ ì„ íƒí•˜ê³  `âŒ¥ Opt + âŒ˜ Cmd + C`ë¥¼ ëˆ„ë¥´ë©´ ê²½ë¡œê°€ ë³µì‚¬ë©ë‹ˆë‹¤."
        )
        
        if st.button("ğŸ”„ ê²½ë¡œ í™•ì¸ ë° ì—°ê²°"):
            root_path = Path(cloud_root_input.strip().strip('"').strip("'"))
            
            # ê²€ì¦: í•„ìˆ˜ í•˜ìœ„ í´ë” ì¡´ì¬ ì—¬ë¶€
            check_inbox = root_path / "01_Library/inbox"
            check_db = root_path / "02_Brain/vector_db"
            
            if check_inbox.exists() and check_db.exists():
                # .env ì—…ë°ì´íŠ¸ ë¡œì§
                try:
                    env_lines = []
                    if ENV_FILE.exists():
                        env_lines = ENV_FILE.read_text(encoding='utf-8').splitlines()
                    
                    new_lines = []
                    keys_updated = {"INBOX_DIR": False, "ARCHIVE_DIR": False, "DB_PATH": False}
                    
                    for line in env_lines:
                        if line.startswith("INBOX_DIR="):
                            new_lines.append(f"INBOX_DIR={check_inbox}")
                            keys_updated["INBOX_DIR"] = True
                        elif line.startswith("ARCHIVE_DIR="):
                            new_lines.append(f"ARCHIVE_DIR={root_path}/01_Library/archive")
                            keys_updated["ARCHIVE_DIR"] = True
                        elif line.startswith("DB_PATH=") or line.startswith("CHROMA_DB_DIR="): # êµ¬ë²„ì „ í˜¸í™˜
                            new_lines.append(f"DB_PATH={check_db}")
                            keys_updated["DB_PATH"] = True
                        else:
                            new_lines.append(line)
                    
                    # ì—†ëŠ” í‚¤ ì¶”ê°€
                    if not keys_updated["INBOX_DIR"]:
                        new_lines.append(f"INBOX_DIR={check_inbox}")
                    if not keys_updated["ARCHIVE_DIR"]:
                        new_lines.append(f"ARCHIVE_DIR={root_path}/01_Library/archive")
                    if not keys_updated["DB_PATH"]:
                        new_lines.append(f"DB_PATH={check_db}")
                        
                    ENV_FILE.write_text("\n".join(new_lines), encoding='utf-8')
                    
                    st.toast("âœ… ê²½ë¡œê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!", icon="ğŸ‰")
                    st.success("ì„¤ì •ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ì ìš©ì„ ìœ„í•´ ì•±ì„ ìƒˆë¡œê³ ì¹¨í•©ë‹ˆë‹¤.")
                    time.sleep(1)
                    st.rerun()
                    
                except Exception as e:
                    st.error(f"ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            else:
                st.warning("âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ë¡œì…ë‹ˆë‹¤.")
                if not check_inbox.exists():
                    st.markdown(f"- âŒ ì°¾ì„ ìˆ˜ ì—†ìŒ: `{check_inbox}`")
                if not check_db.exists():
                    st.markdown(f"- âŒ ì°¾ì„ ìˆ˜ ì—†ìŒ: `{check_db}`")
                st.caption("í´ë” êµ¬ì¡°ê°€ `Theology_AI_LAB/01_Library/inbox` í˜•íƒœì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")

    st.markdown("---")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 3. API í‚¤ ê´€ë¦¬ (ìƒì‹œ ë…¸ì¶œ, ê°„ì„­ ë°©ì§€)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("### ğŸ”‘ API í‚¤ ê´€ë¦¬")
    st.caption("ì‚¬ìš©í•  AI ëª¨ë¸ì˜ API í‚¤ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. [Anthropic](https://console.anthropic.com/) | [OpenAI](https://platform.openai.com/api-keys) | [Google Gemini](https://aistudio.google.com/app/apikey)")

    # ì €ì¥ëœ í‚¤ ë¡œë“œ (ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´)
    saved_anthropic_key = settings.get("ANTHROPIC_API_KEY", "")
    saved_openai_key = settings.get("OPENAI_API_KEY", "")
    saved_google_key = settings.get("GOOGLE_API_KEY", "")

    col_api1, col_api2, col_api3 = st.columns(3)
    
    with col_api1:
        anthropic_key = st.text_input(
            "Anthropic API Key",
            value=saved_anthropic_key,
            type="password",
            placeholder="sk-ant-api03-...",
            key="anthropic_key_input",
            help="Claude ëª¨ë¸ ì‚¬ìš© ì‹œ í•„ìš”"
        )
    
    with col_api2:
        openai_key = st.text_input(
            "OpenAI API Key",
            value=saved_openai_key,
            type="password",
            placeholder="sk-proj-...",
            key="openai_key_input",
            help="GPT ëª¨ë¸ ì‚¬ìš© ì‹œ í•„ìš”"
        )

    with col_api3:
        google_key = st.text_input(
            "Google API Key",
            value=saved_google_key,
            type="password",
            placeholder="AIza...",
            key="google_key_input",
            help="Gemini ëª¨ë¸ ì‚¬ìš© ì‹œ í•„ìš”"
        )

    st.markdown("---")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # RAG ëª¨ë¸ ì„¤ì • (ì„ íƒëœ í”„ë¡œë°”ì´ë”ì˜ ëª¨ë¸ë§Œ í‘œì‹œ)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 4. AI ì¶”ë¡  ì—”ì§„ ì„¤ì • (RAG ëª¨ë¸ ì„ íƒ)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("### ğŸ¤– AI ì¶”ë¡  ì—”ì§„ ì„¤ì •")
    st.caption("ì§ˆë¬¸ì— ë‹µë³€í•  ë©”ì¸ AI ëª¨ë¸ì„ ì„ íƒí•©ë‹ˆë‹¤. ìœ„ì—ì„œ ì…ë ¥í•œ API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    # í”„ë¡œë°”ì´ë”ë³„ ëª¨ë¸ ì •ì˜
    MODELS_BY_PROVIDER = {
        "anthropic": {
            "Claude Opus 4.5 (ìµœì‹ )": "claude-opus-4-5-20251101",
            "Claude Sonnet 4": "claude-sonnet-4-20250514",
            "Claude Haiku 4.5 (ë¹ ë¦„)": "claude-haiku-4-5",
            "Claude 3.5 Sonnet": "claude-3-5-sonnet-20241022",
        },
        "openai": {
            "GPT-5.2 (ìµœì‹ )": "gpt-5.2-2025-12-11",
            "GPT-5.1": "gpt-5.1-2025-11-13",
            "GPT-4.1": "gpt-4.1-2025-04-14",
            "GPT-4.1 Mini (ì €ë ´)": "gpt-4.1-mini-2025-04-14",
            "GPT-4o": "gpt-4o",
        },
        "google": {
            "Gemini 3 Pro (ìµœì‹ )": "gemini-3-pro-preview",
            "Gemini 3 Flash": "gemini-3-flash-preview",
            "Gemini 2.5 Pro": "gemini-2.5-pro",
            "Gemini 2.5 Flash (ì¶”ì²œ)": "gemini-2.5-flash",
            "Gemini 2.0 Flash": "gemini-2.0-flash",
        },
    }

    # í”„ë¡œë°”ì´ë” ì„ íƒ (ë¼ë””ì˜¤ ë²„íŠ¼)
    provider = st.radio(
        "ì‚¬ìš©í•  AI í”„ë¡œë°”ì´ë”",
        ["Anthropic (Claude)", "OpenAI (GPT)", "Google (Gemini)"],
        horizontal=True,
        key="api_provider_select"
    )

    # í”„ë¡œë°”ì´ë” ë§¤í•‘
    provider_map = {
        "Anthropic (Claude)": "anthropic",
        "OpenAI (GPT)": "openai",
        "Google (Gemini)": "google",
    }
    selected_provider = provider_map[provider]

    # ì„ íƒëœ í”„ë¡œë°”ì´ë”ì˜ ëª¨ë¸ë§Œ í‘œì‹œ
    model_options = MODELS_BY_PROVIDER[selected_provider]

    current_model = settings.get("RAG_MODEL", "gemini-2.5-pro")
    # í˜„ì¬ ëª¨ë¸ì´ ì„ íƒëœ í”„ë¡œë°”ì´ë”ì— ìˆëŠ”ì§€ í™•ì¸
    current_model_display = next(
        (k for k, v in model_options.items() if v == current_model),
        list(model_options.keys())[0]  # ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ëª¨ë¸
    )

    # í˜„ì¬ ëª¨ë¸ì´ ë¦¬ìŠ¤íŠ¸ì— ì—†ìœ¼ë©´ ì¸ë±ìŠ¤ 0 ì‚¬ìš©
    try:
        model_index = list(model_options.keys()).index(current_model_display)
    except ValueError:
        model_index = 0

    selected_model_display = st.selectbox(
        f"{provider} ëª¨ë¸",
        list(model_options.keys()),
        index=model_index,
        key="rag_model_select"
    )
    selected_model = model_options[selected_model_display]

    max_tokens = st.slider(
        "ìµœëŒ€ ì‘ë‹µ í† í°",
        min_value=1024,
        max_value=8192,
        value=int(settings.get("RAG_MAX_TOKENS", 4096)),
        step=256,
        key="rag_max_tokens"
    )

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ì €ì¥ ë²„íŠ¼
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("---")

    if st.button("ğŸ’¾ ì„¤ì • ì €ì¥ (Save All)", type="primary"):
        new_settings_data = {
            "ANTHROPIC_API_KEY": anthropic_key,
            "OPENAI_API_KEY": openai_key,
            "GOOGLE_API_KEY": google_key,
            "RAG_MODEL": selected_model,
            "RAG_MAX_TOKENS": str(max_tokens),
            "APP_TITLE": app_title_input,
            "OBSIDIAN_VAULT": obsidian_vault_input,
        }
        try:
            # .env ì €ì¥
            save_env_settings(new_settings_data)
            
            # ì˜µì‹œë””ì–¸ ë³¼íŠ¸ ì´ë ¥ ì €ì¥
            if obsidian_vault_input:
                save_vault_history(obsidian_vault_input)
            
            # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
            st.session_state.current_settings = new_settings_data
            
            st.success("âœ… ëª¨ë“  ì„¤ì •ì´ ì•ˆì „í•˜ê²Œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            st.rerun()  # ì¦‰ì‹œ ë°˜ì˜ì„ ìœ„í•´ ì¬ì‹¤í–‰
            
        except Exception as e:
            st.error(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ì‚¬ìš© ê°€ì´ë“œ
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("---")
    st.markdown("### ğŸ“– ì‚¬ìš© ê°€ì´ë“œ")

    GUI_GUIDE_PATH = KIT_ROOT / "docs" / "GUI_GUIDE.md"
    
    if GUI_GUIDE_PATH.exists():
        with st.expander("Cloud Edition ì‚¬ìš© ì„¤ëª…ì„œ ë³´ê¸°", expanded=False):
            guide_content = GUI_GUIDE_PATH.read_text(encoding="utf-8")
            st.markdown(guide_content)
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Claude Desktop ì—°ë™ (ê°„ëµí™”)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.expander("ğŸ”— Claude Desktop ì—°ë™ ì •ë³´", expanded=False):
        st.markdown("""
        **MCP ì„œë²„ ì„¤ì • (claude_desktop_config.json)**
        
        ì´ ì„¤ì •ì„ ì¶”ê°€í•˜ë©´ Claude Desktop ì•±ì—ì„œ ë‚´ ì„œì¬ë¥¼ ì§ì ‘ ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """)
        st.code(f'''{{
  "mcpServers": {{
    "theology-lab": {{
      "command": "python",
      "args": ["{str(KIT_ROOT / "03_System/server.py")}"],
      "env": {{
        "ANTHROPIC_API_KEY": "your-api-key"
      }}
    }}
  }}
}}''', language="json")

    # í˜„ì¬ ìƒíƒœ í‘œì‹œ
    st.markdown("---")
    st.markdown("### ğŸ“Š í˜„ì¬ ìƒíƒœ")

    col1, col2, col3 = st.columns(3)
    with col1:
        if settings.get("ANTHROPIC_API_KEY"):
            st.success("âœ… Anthropic")
        else:
            st.warning("âš ï¸ Anthropic ë¯¸ì„¤ì •")

    with col2:
        if settings.get("OPENAI_API_KEY"):
            st.success("âœ… OpenAI")
        else:
            st.info("â„¹ï¸ OpenAI ë¯¸ì„¤ì •")

    with col3:
        if settings.get("GOOGLE_API_KEY"):
            st.success("âœ… Google")
        else:
            st.info("â„¹ï¸ Google ë¯¸ì„¤ì •")
