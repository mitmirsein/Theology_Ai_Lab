#!/usr/bin/env python3
"""
ğŸ§  Semantic Chunker for Theology AI Lab v5.1
=============================================
LLM ê¸°ë°˜ ì‹œë§¨í‹± ì²­í‚¹ ëª¨ë“ˆ.

í…ìŠ¤íŠ¸ë¥¼ ë…¼ë¦¬ì  ë‹¨ìœ„(ë¬¸ë‹¨, ë…¼ì¦, í•­ëª©)ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤.
í† í° ê¸°ë°˜ ì²­í‚¹ë³´ë‹¤ ì •ë°€í•œ ê²€ìƒ‰ ê²°ê³¼ ì œê³µ.

Dual-Mode ì „ëµ:
- IDE í™˜ê²½: ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ í™œìš© (ë¬´ë£Œ)
- Streamlit ë°°í¬: ë³„ë„ API í˜¸ì¶œ (ìœ ë£Œ ~$0.02/ì±…)
"""

import re
import logging
from typing import List, Dict, Any, Optional, Generator
from dataclasses import dataclass
from pathlib import Path

logger = logging.getLogger("SemanticChunker")


@dataclass
class SemanticChunk:
    """ì‹œë§¨í‹± ì²­í¬ êµ¬ì¡°"""
    content: str
    chunk_type: str  # paragraph, section, entry, argument
    start_index: int
    end_index: int
    metadata: Dict[str, Any]


class SemanticChunker:
    """
    LLM ê¸°ë°˜ ì‹œë§¨í‹± ì²­í‚¹.
    
    í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• í•©ë‹ˆë‹¤:
    - ì‚¬ì „: í‘œì œì–´(lemma) ë‹¨ìœ„
    - êµì˜í•™: ë…¼ì¦/ë¬¸ë‹¨ ë‹¨ìœ„
    - ì£¼ì„: ì„±ê²½ ì ˆ/êµ¬ ë‹¨ìœ„
    """
    
    # êµ¬ì¡° ê¸°ë°˜ ë¶„ë¦¬ íŒ¨í„´
    HEADING_PATTERN = re.compile(
        r'^(?:#{1,6}\s+|(?:\d+\.)+\s+|[A-Z]\.\s+|[IVX]+\.\s+)(.+)$',
        re.MULTILINE
    )
    
    # ì‚¬ì „ í‘œì œì–´ íŒ¨í„´ (ê·¸ë¦¬ìŠ¤ì–´, íˆë¸Œë¦¬ì–´, ë¼í‹´ì–´)
    LEMMA_PATTERN = re.compile(
        r'^([Î±-Ï‰Î‘-Î©á¼€-á¿·]+|[×-×ª]+|[a-zA-Z]+)\s*[\(\[]',
        re.MULTILINE
    )
    
    # ë¬¸ë‹¨ êµ¬ë¶„ì
    PARAGRAPH_SEPARATORS = ['\n\n', '\n \n', '\r\n\r\n']
    
    def __init__(self, 
                 llm_provider: str = None,
                 api_key: str = None,
                 use_structure: bool = True):
        """
        Args:
            llm_provider: LLM ì œê³µì (google, openai, anthropic)
            api_key: API í‚¤ (Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©)
            use_structure: êµ¬ì¡° ê¸°ë°˜ ë¶„ë¦¬ ì‚¬ìš© ì—¬ë¶€
        """
        self.llm_provider = llm_provider
        self.api_key = api_key
        self.use_structure = use_structure
        self._llm = None
    
    def chunk(self, 
              text: str, 
              doc_type: str = "dogmatics",
              max_chunk_size: int = 1200,
              metadata_base: Dict[str, Any] = None) -> List[SemanticChunk]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì‹œë§¨í‹± ì²­í¬ë¡œ ë¶„í• .
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            doc_type: ë¬¸ì„œ ìœ í˜• (dogmatics, dictionary, commentary)
            max_chunk_size: ìµœëŒ€ ì²­í¬ í¬ê¸° (í† í°)
            metadata_base: ê¸°ë³¸ ë©”íƒ€ë°ì´í„°
            
        Returns:
            SemanticChunk ë¦¬ìŠ¤íŠ¸
        """
        metadata_base = metadata_base or {}
        
        logger.info(f"ğŸ§  Semantic chunking: {len(text)} chars, type={doc_type}")
        
        # 1. ë¬¸ì„œ ìœ í˜•ë³„ ì „ëµ ì„ íƒ
        if doc_type == "dictionary":
            chunks = self._chunk_dictionary(text, max_chunk_size)
        elif doc_type == "commentary":
            chunks = self._chunk_commentary(text, max_chunk_size)
        else:
            chunks = self._chunk_dogmatics(text, max_chunk_size)
        
        # 2. ë©”íƒ€ë°ì´í„° ì£¼ì…
        for i, chunk in enumerate(chunks):
            chunk.metadata.update(metadata_base)
            chunk.metadata["chunk_index"] = i
            chunk.metadata["chunk_type"] = chunk.chunk_type
        
        logger.info(f"   â””â”€ Generated {len(chunks)} chunks")
        return chunks
    
    def _chunk_dictionary(self, text: str, max_size: int) -> List[SemanticChunk]:
        """ì‚¬ì „: í‘œì œì–´ ë‹¨ìœ„ ë¶„í• """
        chunks = []
        
        # í‘œì œì–´ íŒ¨í„´ìœ¼ë¡œ ë¶„í• 
        entries = self._split_by_lemma(text)
        
        for entry_text, lemma in entries:
            if len(entry_text.strip()) < 50:
                continue
            
            # ë„ˆë¬´ ê¸´ í•­ëª©ì€ ì¶”ê°€ ë¶„í• 
            if len(entry_text) > max_size * 4:
                sub_chunks = self._split_by_paragraph(entry_text, max_size)
                for j, sub in enumerate(sub_chunks):
                    chunks.append(SemanticChunk(
                        content=sub,
                        chunk_type="entry",
                        start_index=text.find(sub[:50]),
                        end_index=text.find(sub[:50]) + len(sub),
                        metadata={"lemma": lemma, "sub_index": j}
                    ))
            else:
                chunks.append(SemanticChunk(
                    content=entry_text,
                    chunk_type="entry",
                    start_index=text.find(entry_text[:50]),
                    end_index=text.find(entry_text[:50]) + len(entry_text),
                    metadata={"lemma": lemma}
                ))
        
        return chunks if chunks else self._chunk_dogmatics(text, max_size)
    
    def _chunk_commentary(self, text: str, max_size: int) -> List[SemanticChunk]:
        """ì£¼ì„: ì„±ê²½ ì ˆ ë‹¨ìœ„ ë¶„í• """
        chunks = []
        
        # ì„±ê²½ ì°¸ì¡° íŒ¨í„´ (ì˜ˆ: "1:1", "v. 1", "verse 1")
        verse_pattern = re.compile(
            r'(?:^|\n)(?:(?:\d+:)?\d+\.?\s+|v\.?\s*\d+|verse\s+\d+)',
            re.IGNORECASE | re.MULTILINE
        )
        
        parts = verse_pattern.split(text)
        refs = verse_pattern.findall(text)
        
        for i, part in enumerate(parts):
            if len(part.strip()) < 30:
                continue
            
            ref = refs[i-1].strip() if i > 0 and i <= len(refs) else ""
            
            # ê¸´ ë¶€ë¶„ì€ ë¬¸ë‹¨ìœ¼ë¡œ ì¶”ê°€ ë¶„í• 
            if len(part) > max_size * 4:
                sub_chunks = self._split_by_paragraph(part, max_size)
                for j, sub in enumerate(sub_chunks):
                    chunks.append(SemanticChunk(
                        content=sub,
                        chunk_type="verse",
                        start_index=text.find(sub[:50]) if len(sub) > 50 else 0,
                        end_index=0,
                        metadata={"verse_ref": ref, "sub_index": j}
                    ))
            else:
                chunks.append(SemanticChunk(
                    content=part.strip(),
                    chunk_type="verse",
                    start_index=text.find(part[:50]) if len(part) > 50 else 0,
                    end_index=0,
                    metadata={"verse_ref": ref}
                ))
        
        return chunks if chunks else self._chunk_dogmatics(text, max_size)
    
    def _chunk_dogmatics(self, text: str, max_size: int) -> List[SemanticChunk]:
        """êµì˜í•™: ë…¼ì¦/ë¬¸ë‹¨ ë‹¨ìœ„ ë¶„í• """
        chunks = []
        
        # 1. ë¨¼ì € í—¤ë”©ìœ¼ë¡œ ë¶„í•  ì‹œë„
        sections = self._split_by_heading(text)
        
        for section_text, heading in sections:
            # ê° ì„¹ì…˜ì„ ë¬¸ë‹¨ìœ¼ë¡œ ë¶„í• 
            paragraphs = self._split_by_paragraph(section_text, max_size)
            
            for j, para in enumerate(paragraphs):
                if len(para.strip()) < 50:
                    continue
                
                chunks.append(SemanticChunk(
                    content=para,
                    chunk_type="paragraph" if not heading else "section",
                    start_index=text.find(para[:50]) if len(para) > 50 else 0,
                    end_index=0,
                    metadata={"section_heading": heading, "para_index": j}
                ))
        
        return chunks
    
    def _split_by_lemma(self, text: str) -> List[tuple]:
        """í‘œì œì–´ ë‹¨ìœ„ë¡œ ë¶„í• """
        entries = []
        
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±: ê·¸ë¦¬ìŠ¤/íˆë¸Œë¦¬ì–´ë¡œ ì‹œì‘í•˜ëŠ” ì¤„
        lines = text.split('\n')
        current_entry = []
        current_lemma = ""
        
        for line in lines:
            # ìƒˆ í‘œì œì–´ ì‹œì‘?
            match = self.LEMMA_PATTERN.match(line)
            if match:
                # ì´ì „ í•­ëª© ì €ì¥
                if current_entry:
                    entries.append(('\n'.join(current_entry), current_lemma))
                current_entry = [line]
                current_lemma = match.group(1)
            else:
                current_entry.append(line)
        
        # ë§ˆì§€ë§‰ í•­ëª©
        if current_entry:
            entries.append(('\n'.join(current_entry), current_lemma))
        
        return entries if entries else [(text, "")]
    
    def _split_by_heading(self, text: str) -> List[tuple]:
        """í—¤ë”© ë‹¨ìœ„ë¡œ ë¶„í• """
        sections = []
        
        matches = list(self.HEADING_PATTERN.finditer(text))
        
        if not matches:
            return [(text, "")]
        
        for i, match in enumerate(matches):
            start = match.end()
            end = matches[i + 1].start() if i + 1 < len(matches) else len(text)
            heading = match.group(1).strip()
            section_text = text[start:end].strip()
            
            if section_text:
                sections.append((section_text, heading))
        
        # ì²« í—¤ë”© ì´ì „ ë‚´ìš©
        if matches and matches[0].start() > 0:
            pre_text = text[:matches[0].start()].strip()
            if pre_text:
                sections.insert(0, (pre_text, ""))
        
        return sections if sections else [(text, "")]
    
    def _split_by_paragraph(self, text: str, max_size: int) -> List[str]:
        """ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„í•  (í¬ê¸° ì œí•œ ì ìš©)"""
        # ë¬¸ë‹¨ ë¶„ë¦¬
        paragraphs = re.split(r'\n\s*\n', text)
        
        result = []
        current = ""
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
            
            # í˜„ì¬ + ìƒˆ ë¬¸ë‹¨ì´ ì œí•œì„ ì´ˆê³¼?
            if len(current) + len(para) > max_size * 4:  # ëŒ€ëµì  í† í° ì¶”ì •
                if current:
                    result.append(current)
                current = para
            else:
                current = current + "\n\n" + para if current else para
        
        if current:
            result.append(current)
        
        return result


# ============================================================
# Convenience Function
# ============================================================
def semantic_chunk(text: str, 
                   doc_type: str = "dogmatics",
                   max_size: int = 1200) -> List[Dict[str, Any]]:
    """
    ì‹œë§¨í‹± ì²­í‚¹ í¸ì˜ í•¨ìˆ˜.
    
    Args:
        text: ì›ë³¸ í…ìŠ¤íŠ¸
        doc_type: ë¬¸ì„œ ìœ í˜•
        max_size: ìµœëŒ€ ì²­í¬ í¬ê¸°
        
    Returns:
        ì²­í¬ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
    """
    chunker = SemanticChunker()
    chunks = chunker.chunk(text, doc_type, max_size)
    
    return [
        {
            "content": c.content,
            "chunk_type": c.chunk_type,
            "start_index": c.start_index,
            "metadata": c.metadata
        }
        for c in chunks
    ]


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    test_dict = """
á¼€Î³Î¬Ï€Î· (agapÄ“) - love, divine love
    1. Etymology and Usage
    The term á¼€Î³Î¬Ï€Î· appears throughout the NT...
    
    2. Theological Significance
    In contrast to eros and philia...

Î»ÏŒÎ³Î¿Ï‚ (logos) - word, reason
    1. Background
    The concept of logos has philosophical roots...
    """
    
    test_dogma = """
# Chapter 1: The Doctrine of God

## 1.1 The Being of God

God's being is not a static substance but a living act...

## 1.2 The Trinity

The doctrine of the Trinity affirms that God is one essence...
    """
    
    chunker = SemanticChunker()
    
    print("="*60)
    print("ğŸ“š Dictionary Test")
    print("="*60)
    for c in chunker.chunk(test_dict, "dictionary", 500):
        print(f"\n[{c.chunk_type}] {c.metadata}")
        print(c.content[:100] + "...")
    
    print("\n" + "="*60)
    print("ğŸ“– Dogmatics Test")
    print("="*60)
    for c in chunker.chunk(test_dogma, "dogmatics", 500):
        print(f"\n[{c.chunk_type}] {c.metadata}")
        print(c.content[:100] + "...")
