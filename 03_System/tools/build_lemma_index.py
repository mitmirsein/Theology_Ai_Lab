#!/usr/bin/env python3
"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ  build_lemma_index.py â€” Theological Dictionary Index Builder (v2.0)  â”ƒ
â”ƒ                                                                       â”ƒ
â”ƒ  Features:                                                            â”ƒ
â”ƒ    - í™•ì¥ëœ ë©”íƒ€ë°ì´í„° ì¸ë±ì‹± (category, language, related)            â”ƒ
â”ƒ    - by_category, by_source ë³´ì¡° ì¸ë±ìŠ¤ ìƒì„±                          â”ƒ
â”ƒ    - ì¦ë¶„ ì—…ë°ì´íŠ¸ ì§€ì›                                               â”ƒ
â”ƒ                                                                       â”ƒ
â”ƒ  Usage:                                                               â”ƒ
â”ƒ    python build_lemma_index.py                                        â”ƒ
â”ƒ    python build_lemma_index.py --force                                â”ƒ
â”ƒ                                                                       â”ƒ
â”ƒ  Version: 2.0.0                                                       â”ƒ
â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
"""

import argparse
import json
import os
import sys
from datetime import datetime
from pathlib import Path
from collections import defaultdict
from typing import Any, Generator, List, Dict, Set

try:
    from tqdm import tqdm
except ImportError:
    def tqdm(iterable, desc=None, unit=None):
        print(f"Processing: {desc}")
        return iterable


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Œ CONFIGURATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def discover_archive_path() -> Path:
    """ì•„ì¹´ì´ë¸Œ ê²½ë¡œë¥¼ ìŠ¤ë§ˆíŠ¸í•˜ê²Œ íƒì§€"""
    script_dir = Path(__file__).parent.absolute()
    kit_root = script_dir.parent.parent

    # Priority 1: Env Var
    if os.environ.get("ARCHIVE_DIR"):
        return Path(os.environ.get("ARCHIVE_DIR"))

    # Priority 2: Docker í™˜ê²½ (ì»¨í…Œì´ë„ˆ ë‚´)
    docker_path = Path("/app/01_Library/archive")
    if docker_path.exists():
        return docker_path

    # Priority 3: Standard Kit Path
    local_archive = kit_root / "01_Library" / "archive"
    if local_archive.exists():
        return local_archive.resolve()

    # Fallback
    return Path.home() / "Desktop/MS_Brain.nosync/300 Tech/320 Coding/Projects.nosync/Theology_Project.nosync/archive"


ARCHIVE_PATH = discover_archive_path()
INDEX_FILE = ARCHIVE_PATH / "lemma_index.json"
META_FILE = ARCHIVE_PATH / "lemma_index_meta.json"

# Index version for migration
INDEX_VERSION = "2.0"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ› ï¸ HELPER FUNCTIONS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def stream_json_entries(file_path: Path) -> Generator[Dict[str, Any], None, None]:
    """JSON íŒŒì¼ì—ì„œ ì—”íŠ¸ë¦¬ë¥¼ ìŠ¤íŠ¸ë¦¬ë°"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read().strip()
        data = json.loads(content)

        if isinstance(data, list):
            yield from data
        elif isinstance(data, dict):
            for key in ["entries", "items", "data", "articles", "content", "chunks"]:
                if key in data and isinstance(data[key], list):
                    yield from data[key]
                    return
            yield data
    except json.JSONDecodeError:
        pass


def get_entry_field(entry: dict, *keys: str, default: Any = None) -> Any:
    """ì—”íŠ¸ë¦¬ì—ì„œ í•„ë“œ ê°’ ì¶”ì¶œ (metadata ë‚´ë¶€ë„ ê²€ìƒ‰)"""
    for key in keys:
        if key in entry:
            return entry[key]
        if "metadata" in entry and key in entry["metadata"]:
            return entry["metadata"][key]
    return default


def normalize_lemma(lemma: str) -> str:
    """Lemma ì •ê·œí™” (ì†Œë¬¸ì, ê³µë°± ì •ë¦¬)"""
    return lemma.strip().lower()


def load_json(path: Path, default: Any = None) -> Any:
    """JSON íŒŒì¼ ì•ˆì „í•˜ê²Œ ë¡œë“œ"""
    if not path.exists():
        return default
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except json.JSONDecodeError:
        print(f"âš ï¸  Warning: Corrupt JSON file {path}, starting fresh.")
        return default


def save_json(path: Path, data: Any, indent: int = None):
    """JSON íŒŒì¼ ì €ì¥"""
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=indent)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Š INDEX BUILDER (v2.0)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class EnhancedLemmaIndexer:
    """í™•ì¥ëœ Lemma ì¸ë±ì„œ (v2.0)"""

    def __init__(self):
        # ë©”ì¸ ì¸ë±ìŠ¤: lemma -> list of entries
        self.entries: Dict[str, List[Dict]] = defaultdict(list)

        # ë³´ì¡° ì¸ë±ìŠ¤
        self.by_category: Dict[str, Set[str]] = defaultdict(set)
        self.by_source: Dict[str, Dict[str, Any]] = defaultdict(
            lambda: {"count": 0, "volumes": set()}
        )

        # íŒŒì¼ ë©”íƒ€ë°ì´í„° (mtime ì¶”ì )
        self.file_metadata: Dict[str, float] = {}

    def add_entry(
        self,
        lemma: str,
        file: str,
        page: Any,
        source: str = None,
        volume: Any = None,
        category: List[str] = None,
        language: str = None,
        related: List[str] = None,
    ):
        """ì¸ë±ìŠ¤ì— ì—”íŠ¸ë¦¬ ì¶”ê°€"""
        norm_lemma = normalize_lemma(lemma)

        # ë©”ì¸ ì—”íŠ¸ë¦¬
        entry = {
            "file": file,
            "page": page,
        }

        # ì„ íƒì  í•„ë“œ ì¶”ê°€
        if source:
            entry["source"] = source
        if volume:
            entry["volume"] = volume
        if category:
            entry["category"] = category
        if language:
            entry["language"] = language
        if related:
            entry["related"] = related

        self.entries[norm_lemma].append(entry)

        # ë³´ì¡° ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
        if category:
            for cat in category:
                self.by_category[cat].add(norm_lemma)

        if source:
            self.by_source[source]["count"] += 1
            if volume:
                self.by_source[source]["volumes"].add(volume)

    def purge_file(self, filename: str):
        """íŠ¹ì • íŒŒì¼ì˜ ëª¨ë“  ì—”íŠ¸ë¦¬ ì œê±°"""
        for lemma in list(self.entries.keys()):
            self.entries[lemma] = [
                e for e in self.entries[lemma] if e.get("file") != filename
            ]
            if not self.entries[lemma]:
                del self.entries[lemma]

    def rebuild_auxiliary_indices(self):
        """ë³´ì¡° ì¸ë±ìŠ¤ ì¬êµ¬ì¶•"""
        self.by_category.clear()
        self.by_source.clear()

        for lemma, entries in self.entries.items():
            for entry in entries:
                # ì¹´í…Œê³ ë¦¬ ì¸ë±ìŠ¤
                if "category" in entry:
                    for cat in entry["category"]:
                        self.by_category[cat].add(lemma)

                # ì†ŒìŠ¤ ì¸ë±ìŠ¤
                source = entry.get("source")
                if source:
                    self.by_source[source]["count"] += 1
                    volume = entry.get("volume")
                    if volume:
                        self.by_source[source]["volumes"].add(volume)

    def to_dict(self) -> Dict[str, Any]:
        """ì§ë ¬í™” ê°€ëŠ¥í•œ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        # Setì„ Listë¡œ ë³€í™˜
        by_category_serializable = {
            cat: sorted(list(lemmas)) for cat, lemmas in self.by_category.items()
        }

        by_source_serializable = {}
        for source, data in self.by_source.items():
            by_source_serializable[source] = {
                "count": data["count"],
                "volumes": sorted(list(data["volumes"])),
            }

        return {
            "version": INDEX_VERSION,
            "updated_at": datetime.now().isoformat(),
            "entries": dict(self.entries),
            "by_category": by_category_serializable,
            "by_source": by_source_serializable,
        }

    def from_dict(self, data: Dict[str, Any]):
        """ë”•ì…”ë„ˆë¦¬ì—ì„œ ë¡œë“œ"""
        if not data:
            return

        # ë²„ì „ í™•ì¸
        version = data.get("version", "1.0")

        if version.startswith("2."):
            # v2.0 í˜•ì‹
            self.entries = defaultdict(list, data.get("entries", {}))

            # ë³´ì¡° ì¸ë±ìŠ¤ ë¡œë“œ
            for cat, lemmas in data.get("by_category", {}).items():
                self.by_category[cat] = set(lemmas)

            for source, info in data.get("by_source", {}).items():
                self.by_source[source] = {
                    "count": info.get("count", 0),
                    "volumes": set(info.get("volumes", [])),
                }
        else:
            # v1.0 í˜•ì‹ ë§ˆì´ê·¸ë ˆì´ì…˜ (ê¸°ì¡´ í˜•ì‹)
            print("   ğŸ“¦ v1.0 ì¸ë±ìŠ¤ ê°ì§€. v2.0ìœ¼ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì¤‘...")
            for lemma, entries in data.items():
                if isinstance(entries, list):
                    self.entries[lemma] = entries
            self.rebuild_auxiliary_indices()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸš€ MAIN LOGIC
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    parser = argparse.ArgumentParser(
        description="Build/Update Theological Dictionary Lemma Index (v2.0)"
    )
    parser.add_argument("--force", "-f", action="store_true", help="Force full rebuild")
    parser.add_argument(
        "--pretty", "-p", action="store_true", help="Pretty-print JSON output"
    )
    args = parser.parse_args()

    if not ARCHIVE_PATH.exists():
        print(f"âŒ Archive path not found: {ARCHIVE_PATH}")
        sys.exit(1)

    print("=" * 60)
    print("ğŸ“š Theology AI Lab - Lemma Index Builder v2.0")
    print("=" * 60)
    print(f"ğŸ“‚ Archive: {ARCHIVE_PATH}")

    # ì¸ë±ì„œ ì´ˆê¸°í™”
    indexer = EnhancedLemmaIndexer()

    # ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ
    if not args.force:
        existing_index = load_json(INDEX_FILE, {})
        indexer.from_dict(existing_index)
        indexer.file_metadata = load_json(META_FILE, {})
        print(f"   ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ: {len(indexer.entries)} lemmas")
    else:
        print("   ê°•ì œ ì¬ë¹Œë“œ ëª¨ë“œ")

    # í˜„ì¬ íŒŒì¼ ìŠ¤ìº”
    current_files = {
        f.name: f
        for f in ARCHIVE_PATH.glob("*.json")
        if f.name not in ["lemma_index.json", "lemma_index_meta.json"]
    }

    # ë³€ê²½ ê°ì§€
    new_or_modified = []
    deleted_files = set(indexer.file_metadata.keys()) - set(current_files.keys())

    for filename, filepath in current_files.items():
        mtime = filepath.stat().st_mtime
        if filename not in indexer.file_metadata or indexer.file_metadata[filename] != mtime:
            new_or_modified.append(filepath)

    # ë³€ê²½ ì—†ìœ¼ë©´ ì¢…ë£Œ
    if not new_or_modified and not deleted_files and not args.force and indexer.entries:
        print("\nâœ… ì¸ë±ìŠ¤ê°€ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤!")
        sys.exit(0)

    print(f"\n   ë³€ê²½ ê°ì§€: {len(new_or_modified)} ì‹ ê·œ/ìˆ˜ì •, {len(deleted_files)} ì‚­ì œ")

    # 1. ì‚­ì œëœ/ìˆ˜ì •ëœ íŒŒì¼ ì—”íŠ¸ë¦¬ ì œê±°
    files_to_purge = deleted_files | {f.name for f in new_or_modified}

    if files_to_purge:
        print("\nğŸ§¹ ê¸°ì¡´ ì—”íŠ¸ë¦¬ ì •ë¦¬ ì¤‘...")
        for filename in files_to_purge:
            indexer.purge_file(filename)
            if filename in indexer.file_metadata:
                del indexer.file_metadata[filename]

    # 2. ì‹ ê·œ/ìˆ˜ì • íŒŒì¼ ì²˜ë¦¬
    if new_or_modified:
        print(f"\nğŸ“¦ {len(new_or_modified)} íŒŒì¼ ì¸ë±ì‹± ì¤‘...")

        for file_path in tqdm(new_or_modified, unit="file"):
            filename = file_path.name
            file_entries = 0

            try:
                for entry in stream_json_entries(file_path):
                    # í•„ìˆ˜ í•„ë“œ
                    raw_lemma = get_entry_field(
                        entry, "lemma", "headword", "title", "Lemma", "entry"
                    )
                    page = get_entry_field(entry, "page_number", "page", "Page", "seite")

                    if raw_lemma:
                        # í™•ì¥ í•„ë“œ ì¶”ì¶œ
                        source = get_entry_field(entry, "source", "dict", "dictionary")
                        volume = get_entry_field(entry, "volume", "vol", "band")
                        category = get_entry_field(entry, "category", "categories")
                        language = get_entry_field(entry, "language", "lang")
                        related = get_entry_field(entry, "related_lemmas", "related")

                        # ì¹´í…Œê³ ë¦¬ê°€ ë¬¸ìì—´ì´ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                        if isinstance(category, str):
                            category = [category]

                        indexer.add_entry(
                            lemma=str(raw_lemma),
                            file=filename,
                            page=page,
                            source=source,
                            volume=volume,
                            category=category,
                            language=language,
                            related=related,
                        )
                        file_entries += 1

                # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
                indexer.file_metadata[filename] = file_path.stat().st_mtime

            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜: {filename}: {e}")

    # 3. ë³´ì¡° ì¸ë±ìŠ¤ ì¬êµ¬ì¶•
    print("\nğŸ“Š ë³´ì¡° ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ì¤‘...")
    indexer.rebuild_auxiliary_indices()

    # 4. ì €ì¥
    print("\nğŸ’¾ ì¸ë±ìŠ¤ ì €ì¥ ì¤‘...")
    indent = 2 if args.pretty else None
    save_json(INDEX_FILE, indexer.to_dict(), indent=indent)
    save_json(META_FILE, indexer.file_metadata)

    # 5. í†µê³„ ì¶œë ¥
    print("\n" + "=" * 60)
    print("âœ… ì¸ë±ì‹± ì™„ë£Œ!")
    print("=" * 60)
    print(f"   ğŸ“ ì´ Lemma ìˆ˜: {len(indexer.entries):,}")
    print(f"   ğŸ“‚ ì¹´í…Œê³ ë¦¬: {len(indexer.by_category)}")
    print(f"   ğŸ“š ì‚¬ì „/ì†ŒìŠ¤: {len(indexer.by_source)}")

    if indexer.by_category:
        print("\n   ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:")
        for cat, lemmas in sorted(indexer.by_category.items(), key=lambda x: -len(x[1]))[:5]:
            print(f"      - {cat}: {len(lemmas)}")

    if indexer.by_source:
        print("\n   ì†ŒìŠ¤ë³„ ë¶„í¬:")
        for source, info in sorted(
            indexer.by_source.items(), key=lambda x: -x[1]["count"]
        )[:5]:
            vols = f" (Vol. {', '.join(map(str, sorted(info['volumes'])))})" if info["volumes"] else ""
            print(f"      - {source}: {info['count']}{vols}")

    print("=" * 60)


if __name__ == "__main__":
    main()
